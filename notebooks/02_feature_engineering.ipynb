{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BTC Feature Engineering - CORRECTED APPROACH\n",
        "\n",
        "## Overview\n",
        "This notebook implements the CORRECTED approach: Calculate everything together, split on save.\n",
        "\n",
        "**Key Changes:**\n",
        "1. Load full data (including buffer) for complete calculations\n",
        "2. Calculate all indicators and features on full data\n",
        "3. Create feature sets A0â†’A4 with proper temporal alignment\n",
        "4. Split clean data only at save step\n",
        "\n",
        "**Benefits:**\n",
        "- Complete historical context for all calculations\n",
        "- Proper temporal alignment with full data\n",
        "- No missing data for lag features\n",
        "- Clean final output without buffer data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import talib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "\n",
        "train_start='2020-05-12'\n",
        "test_end='2025-09-19'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Load Full Data (Including Buffer) for Complete Calculations\n",
        "def load_full_data():\n",
        "    \"\"\"Load full data including buffer for complete calculations\"\"\"\n",
        "    \n",
        "    # Load full data\n",
        "    h4_full = pd.read_parquet('../data_collection/data/btc_4h_20251028.parquet')\n",
        "    d1_full = pd.read_parquet('../data_collection/data/btc_1d_20251028.parquet')\n",
        "    w1_full = pd.read_parquet('../data_collection/data/btc_1w_20251028.parquet')\n",
        "    # m1_full = pd.read_parquet('../data_collection/data/btc_1M_20251022.parquet')\n",
        "    \n",
        "    # Ensure datetime index\n",
        "    for df in [h4_full, d1_full, w1_full]:\n",
        "        df.index = pd.to_datetime(df.index)\n",
        "    \n",
        "    print(f\"ðŸ“Š Full data loaded (including buffer):\")\n",
        "    print(f\"  H4: {len(h4_full)} records ({h4_full.index[0]} to {h4_full.index[-1]})\")\n",
        "    print(f\"  D1: {len(d1_full)} records ({d1_full.index[0]} to {d1_full.index[-1]})\")\n",
        "    print(f\"  W1: {len(w1_full)} records ({w1_full.index[0]} to {w1_full.index[-1]})\")\n",
        "    # print(f\"  M1: {len(m1_full)} records ({m1_full.index[0]} to {m1_full.index[-1]})\")\n",
        "    \n",
        "    return h4_full, d1_full, w1_full\n",
        "\n",
        "# Load full data for complete calculations\n",
        "h4_full, d1_full, w1_full = load_full_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Technical Indicator Functions\n",
        "def extract_ohlcv_features(data):\n",
        "    \"\"\"Extract OHLCV features (5 features)\"\"\"\n",
        "    features = pd.DataFrame(index=data.index)\n",
        "    features['open'] = data['open']\n",
        "    features['high'] = data['high']\n",
        "    features['low'] = data['low']\n",
        "    features['close'] = data['close']\n",
        "    features['volume'] = data['volume']\n",
        "    features['volume_MA_20'] = talib.SMA(data['volume'], timeperiod=20)\n",
        "    return features\n",
        "\n",
        "def calculate_moving_averages(data, periods=[7, 14, 20, 60, 120]):\n",
        "    \"\"\"Calculate moving averages using CLOSE prices (5 features)\"\"\"\n",
        "    features = pd.DataFrame(index=data.index)\n",
        "    for period in periods:\n",
        "        features[f'MA_{period}'] = talib.SMA(data['close'], timeperiod=period)\n",
        "    return features\n",
        "\n",
        "def calculate_rsi(data, period=14):\n",
        "    \"\"\"Calculate RSI using CLOSE prices (1 feature)\"\"\"\n",
        "    features = pd.DataFrame(index=data.index)\n",
        "    features['RSI_14'] = talib.RSI(data['close'], timeperiod=period)\n",
        "    return features\n",
        "\n",
        "def calculate_macd(data, fast=12, slow=26, signal=9):\n",
        "    \"\"\"Calculate MACD line, signal, and histogram (3 features)\"\"\"\n",
        "    features = pd.DataFrame(index=data.index)\n",
        "    macd_line, macd_signal, macd_hist = talib.MACD(data['close'], \n",
        "                                                   fastperiod=fast, \n",
        "                                                   slowperiod=slow, \n",
        "                                                   signalperiod=signal)\n",
        "    features['MACD_line'] = macd_line\n",
        "    features['MACD_signal'] = macd_signal\n",
        "    features['MACD_hist'] = macd_hist\n",
        "    return features\n",
        "\n",
        "def calculate_ichimoku(data):\n",
        "    \"\"\"Calculate Ichimoku Cloud components (5 features)\"\"\"\n",
        "    features = pd.DataFrame(index=data.index)\n",
        "    \n",
        "    # Tenkan-sen (Conversion Line)\n",
        "    high_9 = data['high'].rolling(window=9).max()\n",
        "    low_9 = data['low'].rolling(window=9).min()\n",
        "    features['conversion_line'] = (high_9 + low_9) / 2\n",
        "    \n",
        "    # Kijun-sen (Baseline)\n",
        "    high_26 = data['high'].rolling(window=26).max()\n",
        "    low_26 = data['low'].rolling(window=26).min()\n",
        "    features['baseline'] = (high_26 + low_26) / 2\n",
        "    \n",
        "    # Senkou Span A (Leading Span A)\n",
        "    span_a_value = (features['conversion_line'] + features['baseline']) / 2\n",
        "    features['leading_span_A'] = span_a_value.shift(26)\n",
        "    # features['leading_span_A'] = (features['conversion_line'] + features['baseline']) / 2\n",
        "    \n",
        "    # Senkou Span B (Leading Span B)\n",
        "    high_52 = talib.MAX(data['high'], timeperiod=52)\n",
        "    low_52 = talib.MIN(data['low'], timeperiod=52)\n",
        "    span_b_value = (high_52 + low_52) / 2\n",
        "    features['leading_span_B'] = span_b_value.shift(26)\n",
        "\n",
        "    # high_52 = data['high'].rolling(window=52).max()\n",
        "    # low_52 = data['low'].rolling(window=52).min()\n",
        "    # features['leading_span_B'] = (high_52 + low_52) / 2\n",
        "    \n",
        "    # Chikou Span (Lagging Span) - Current close compared to 26 periods ago\n",
        "    features['lagging_span'] = data['close'].shift(26)\n",
        "    \n",
        "    return features\n",
        "\n",
        "def calculate_all_indicators(data, timeframe_name):\n",
        "    \"\"\"Calculate all 19 indicators for a timeframe\"\"\"\n",
        "    print(f\"Calculating indicators for {timeframe_name}...\")\n",
        "    \n",
        "    # Combine all indicator functions\n",
        "    ohlcv = extract_ohlcv_features(data)\n",
        "    ma = calculate_moving_averages(data)\n",
        "    rsi = calculate_rsi(data)\n",
        "    macd = calculate_macd(data)\n",
        "    ichimoku = calculate_ichimoku(data)\n",
        "    \n",
        "    # Combine all features\n",
        "    all_features = pd.concat([ohlcv, ma, rsi, macd, ichimoku], axis=1)\n",
        "    \n",
        "    # Add timeframe prefix to column names\n",
        "    # all_features.columns = [f\"{timeframe_name}_{col}\" for col in all_features.columns]\n",
        "    \n",
        "    print(f\"âœ… {timeframe_name}: {len(all_features.columns)} features created\")\n",
        "    return all_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from typing import List, Optional\n",
        "from itertools import combinations\n",
        "\n",
        "def normalize_moving_averages(data:pd.DataFrame,periods:Optional=[7,14,20,60,120])->pd.DataFrame:\n",
        "    \"\"\"Normalize moving averages by dividing by close price\"\"\"\n",
        "    new_features = {}\n",
        "\n",
        "    for period in periods:\n",
        "        if f'MA_{period}' not in data.columns:\n",
        "            print(f\"Warning: {f'MA_{period}'} not found. Skipping.\")\n",
        "            continue\n",
        "        ma_col = f'MA_{period}'\n",
        "        new_feature_name = f'{ma_col}_norm'\n",
        "        normalized_val = (data['close'] / data[ma_col]) - 1\n",
        "        new_features[new_feature_name] = normalized_val.replace([np.inf, -np.inf], 0)\n",
        "    for (p_short,p_long) in combinations(periods,2):\n",
        "        if f'MA_{p_short}' not in data.columns or f'MA_{p_long}' not in data.columns:\n",
        "            print(f\"Warning: {f'MA_{p_short}'} or {f'MA_{p_long}'} not found. Skipping.\")\n",
        "            continue\n",
        "        ma_col_short = f'MA_{p_short}'\n",
        "        ma_col_long = f'MA_{p_long}'\n",
        "        new_feature_name = f'{ma_col_short}_{ma_col_long}_norm'\n",
        "        normalized_val = (data[ma_col_short] / data[ma_col_long]) - 1\n",
        "        new_features[new_feature_name] = normalized_val.replace([np.inf, -np.inf], 0)\n",
        "\n",
        "    for col_name,values in new_features.items():\n",
        "        data[col_name] = values\n",
        "\n",
        "    return data\n",
        "\n",
        "def normalize_ichimoku(data: pd.DataFrame) -> pd.DataFrame:\n",
        "    new_features = {}\n",
        "    \n",
        "    # ì›ë³¸ Ichimoku ë¼ì¸ ì´ë¦„ ëª©ë¡\n",
        "    ichimoku_lines = [\n",
        "        'conversion_line', \n",
        "        'baseline', \n",
        "        'leading_span_A', \n",
        "        'leading_span_B', \n",
        "        'lagging_span'\n",
        "    ]\n",
        "    \n",
        "    # --- 1. (Close vs. ì›ë³¸ Ichimoku Line) ê³„ì‚° ---\n",
        "    for line_col in ichimoku_lines:\n",
        "        if line_col not in data.columns:\n",
        "            print(f\"Warning: {line_col} not found. Skipping.\")\n",
        "            continue\n",
        "            \n",
        "        new_feature_name = f'close_vs_{line_col}_pct'\n",
        "        # (close / line) - 1\n",
        "        normalized_val = (data['close'] / data[line_col]) - 1\n",
        "        new_features[new_feature_name] = normalized_val.replace([np.inf, -np.inf, np.nan], 0) # NaNë„ 0ìœ¼ë¡œ ì²˜ë¦¬\n",
        "\n",
        "    # --- 2. (ì›ë³¸ Line vs. ì›ë³¸ Line) ê³„ì‚° (í¬ë¡œìŠ¤) ---\n",
        "    \n",
        "    # ì „í™˜ì„  vs ê¸°ì¤€ì„ \n",
        "    if 'conversion_line' in data.columns and 'baseline' in data.columns:\n",
        "        new_feature_name = 'conversion_vs_baseline_pct'\n",
        "        # (conversion / baseline) - 1\n",
        "        normalized_val = (data['conversion_line'] / data['baseline']) - 1\n",
        "        new_features[new_feature_name] = normalized_val.replace([np.inf, -np.inf, np.nan], 0)\n",
        "\n",
        "    # ì„ í–‰ìŠ¤íŒ¬ A vs ì„ í–‰ìŠ¤íŒ¬ B (êµ¬ë¦„ ê´€ê³„)\n",
        "    if 'leading_span_A' in data.columns and 'leading_span_B' in data.columns:\n",
        "        new_feature_name = 'span_A_vs_span_B_pct'\n",
        "        # (Span A / Span B) - 1\n",
        "        normalized_val = (data['leading_span_A'] / data['leading_span_B']) - 1\n",
        "        new_features[new_feature_name] = normalized_val.replace([np.inf, -np.inf, np.nan], 0)\n",
        "\n",
        "    # --- 3. ê³„ì‚°ëœ ëª¨ë“  í”¼ì²˜ë¥¼ ì›ë³¸ DataFrameì— í•œ ë²ˆì— ì¶”ê°€ ---\n",
        "    for col_name, values in new_features.items():\n",
        "        data[col_name] = values\n",
        "            \n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "def normalize_candle_features(data: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    OHLC ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ ì •ê·œí™”ëœ ìº”ë“¤ ëª¨ì–‘ í”¼ì²˜ë¥¼ ê³„ì‚°í•˜ì—¬ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
        "\n",
        "    Args:\n",
        "        data: 'open', 'high', 'low', 'close' ì»¬ëŸ¼ì´ í¬í•¨ëœ DataFrame\n",
        "\n",
        "    Returns:\n",
        "        ì •ê·œí™”ëœ ìº”ë“¤ í”¼ì²˜('candle_body_pct', 'high_wick_pct',\n",
        "        'low_wick_pct', 'range_pct')ê°€ ì¶”ê°€ëœ DataFrame\n",
        "    \"\"\"\n",
        "    # ê³„ì‚°ëœ ìƒˆ í”¼ì²˜ë“¤ì„ ìž„ì‹œ ì €ìž¥\n",
        "    new_features = {}\n",
        "\n",
        "    # 1. ìº”ë“¤ ëª¸í†µ ë¹„ìœ¨ (ì¢…ê°€ ë³€í™”ìœ¨)\n",
        "    # (close - open) / open\n",
        "    open_price = data['open']\n",
        "    close_price = data['close']\n",
        "    new_features['candle_body_pct'] = np.where(\n",
        "        open_price != 0,\n",
        "        (close_price - open_price) / open_price,\n",
        "        0\n",
        "    )\n",
        "\n",
        "    # 2. ìœ—ê¼¬ë¦¬ ë¹„ìœ¨\n",
        "    # (high - max(open, close)) / close\n",
        "    high_price = data['high']\n",
        "    body_top = np.maximum(open_price, close_price) # ëª¸í†µ ìƒë‹¨ (ì–‘ë´‰ì´ë©´ close, ìŒë´‰ì´ë©´ open)\n",
        "    new_features['high_wick_pct'] = np.where(\n",
        "        close_price != 0,\n",
        "        (high_price - body_top) / close_price,\n",
        "        0\n",
        "    )\n",
        "\n",
        "    # 3. ì•„ëž«ê¼¬ë¦¬ ë¹„ìœ¨\n",
        "    # (min(open, close) - low) / close\n",
        "    low_price = data['low']\n",
        "    body_bottom = np.minimum(open_price, close_price) # ëª¸í†µ í•˜ë‹¨ (ì–‘ë´‰ì´ë©´ open, ìŒë´‰ì´ë©´ close)\n",
        "    new_features['low_wick_pct'] = np.where(\n",
        "        close_price != 0,\n",
        "        (body_bottom - low_price) / close_price,\n",
        "        0\n",
        "    )\n",
        "\n",
        "    # 4. ìº”ë“¤ ì „ì²´ ë²”ìœ„ ë¹„ìœ¨\n",
        "    # (high - low) / low\n",
        "    new_features['range_pct'] = np.where(\n",
        "        low_price != 0,\n",
        "        (high_price - low_price) / low_price,\n",
        "        0\n",
        "    )\n",
        "\n",
        "    # ê³„ì‚°ëœ ëª¨ë“  í”¼ì²˜ë¥¼ ì›ë³¸ DataFrameì— í•œ ë²ˆì— ì¶”ê°€\n",
        "    for col_name, values in new_features.items():\n",
        "        data[col_name] = values\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def normalize_volume_features(data: pd.DataFrame, ma_periods: Optional[List[int]] = [20]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Volume í”¼ì²˜ë¥¼ ë‘ ê°€ì§€ ë°©ì‹ìœ¼ë¡œ ì •ê·œí™”í•˜ì—¬ ìƒˆ ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
        "    1. (í˜„ìž¬ Volume vs. Volume MA) ì´ê²©ë„\n",
        "    2. (ì´ì „ ìº”ë“¤ ëŒ€ë¹„ Volume ë³€í™”ìœ¨)\n",
        "\n",
        "    Args:\n",
        "        data: 'volume' ë° ê³„ì‚°ëœ Volume MA ì»¬ëŸ¼('volume_MA_20' ë“±)ì´ í¬í•¨ëœ DataFrame\n",
        "        ma_periods: Volume MA ê³„ì‚°ì— ì‚¬ìš©ëœ ê¸°ê°„ ëª©ë¡\n",
        "\n",
        "    Returns:\n",
        "        ì •ê·œí™”ëœ Volume í”¼ì²˜('volume_vs_MA_20_pct', 'volume_change_pct')ê°€ ì¶”ê°€ëœ DataFrame\n",
        "    \"\"\"\n",
        "\n",
        "    # ê³„ì‚°ëœ ìƒˆ í”¼ì²˜ë“¤ì„ ìž„ì‹œ ì €ìž¥\n",
        "    new_features = {}\n",
        "\n",
        "    # --- 1. (Volume vs. Volume MA) ì´ê²©ë„ ê³„ì‚° ---\n",
        "    if 'volume' in data.columns:\n",
        "        for period in ma_periods:\n",
        "            ma_col = f'volume_MA_{period}'\n",
        "            new_feature_name = f'volume_vs_{ma_col}_pct'\n",
        "            \n",
        "            if ma_col in data.columns:\n",
        "                # (volume / volume_MA) - 1\n",
        "                normalized_val = (data['volume'] / data[ma_col]) - 1\n",
        "                new_features[new_feature_name] = normalized_val.replace([np.inf, -np.inf, np.nan], 0)\n",
        "            else:\n",
        "                 print(f\"Warning: {ma_col} not found for normalization. Skipping {new_feature_name}.\")\n",
        "\n",
        "\n",
        "    # --- 2. (ì´ì „ ìº”ë“¤ ëŒ€ë¹„ Volume ë³€í™”ìœ¨) ê³„ì‚° ---\n",
        "    if 'volume' in data.columns:\n",
        "        new_feature_name = 'volume_change_pct'\n",
        "        # .pct_change()ëŠ” ì´ì „ ê°’ ëŒ€ë¹„ ë³€í™”ìœ¨ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
        "        # ì²« ë²ˆì§¸ ê°’ì€ NaNì´ ë˜ë¯€ë¡œ fillna(0)ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n",
        "        new_features[new_feature_name] = data['volume'].pct_change().fillna(0).replace([np.inf, -np.inf], 0)\n",
        "    else:\n",
        "        print(\"Warning: 'volume' column not found. Skipping volume_change_pct.\")\n",
        "\n",
        "\n",
        "    # --- 3. ê³„ì‚°ëœ ëª¨ë“  í”¼ì²˜ë¥¼ ì›ë³¸ DataFrameì— í•œ ë²ˆì— ì¶”ê°€ ---\n",
        "    for col_name, values in new_features.items():\n",
        "        data[col_name] = values\n",
        "\n",
        "    return data\n",
        "\n",
        "def normalize_all_features(data: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Normalize all features\"\"\"\n",
        "    data = normalize_moving_averages(data)\n",
        "    data = normalize_ichimoku(data)\n",
        "    data = normalize_candle_features(data)\n",
        "    data = normalize_volume_features(data)\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "h4_full.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from typing import List, Optional\n",
        "\n",
        "def remove_absolute_value_features(data: pd.DataFrame, \n",
        "                                   ma_periods: Optional[List[int]] = [7, 14, 20, 60, 120],\n",
        "                                   volume_ma_periods: Optional[List[int]] = [20]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Removes the original absolute value columns (OHLC, Volume, MA, Ichimoku) \n",
        "    from the DataFrame, keeping only the normalized features.\n",
        "\n",
        "    Args:\n",
        "        data: DataFrame containing both original and normalized features.\n",
        "        ma_periods: List of periods used for price moving averages.\n",
        "        volume_ma_periods: List of periods used for volume moving averages.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with absolute value columns removed.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Columns to potentially remove\n",
        "    cols_to_remove = []\n",
        "\n",
        "    # 1. Original OHLC\n",
        "    cols_to_remove.extend(['open', 'high', 'low', 'close'])\n",
        "\n",
        "    # 2. Original Volume\n",
        "    cols_to_remove.append('volume')\n",
        "\n",
        "    # 3. Original Price Moving Averages\n",
        "    for period in ma_periods:\n",
        "        cols_to_remove.append(f'MA_{period}')\n",
        "\n",
        "    # 4. Original Ichimoku Lines\n",
        "    cols_to_remove.extend([\n",
        "        'conversion_line', \n",
        "        'baseline', \n",
        "        'leading_span_A', \n",
        "        'leading_span_B', \n",
        "        'lagging_span'\n",
        "    ])\n",
        "    \n",
        "    # 5. Original Volume Moving Averages\n",
        "    for period in volume_ma_periods:\n",
        "        cols_to_remove.append(f'volume_MA_{period}')\n",
        "\n",
        "    # Check which columns actually exist in the DataFrame\n",
        "    existing_cols_to_remove = [col for col in cols_to_remove if col in data.columns]\n",
        "    \n",
        "    # Drop the existing columns\n",
        "    print(f\"Removing {len(existing_cols_to_remove)} absolute value columns: {existing_cols_to_remove}\")\n",
        "    data = data.drop(columns=existing_cols_to_remove)\n",
        "    \n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Calculate Indicators on Full Data (Including Buffer)\n",
        "def calculate_indicators_on_full_data(h4_full, d1_full, w1_full):\n",
        "    \"\"\"Calculate indicators using full data to ensure proper calculations\"\"\"\n",
        "    \n",
        "    print(\"ðŸ”„ Calculating indicators on full data (including buffer)...\")\n",
        "    \n",
        "    # Calculate indicators on full data\n",
        "    h4_indicators_full = calculate_all_indicators(h4_full, 'H4')\n",
        "    d1_indicators_full = calculate_all_indicators(d1_full, 'D1')\n",
        "    w1_indicators_full = calculate_all_indicators(w1_full, 'W1')\n",
        "    print(h4_indicators_full.columns)\n",
        "    print(d1_indicators_full.columns)\n",
        "    print(w1_indicators_full.columns)\n",
        "\n",
        "    # normalize\n",
        "    h4_indicators_full = normalize_all_features(h4_indicators_full)\n",
        "    d1_indicators_full = normalize_all_features(d1_indicators_full)\n",
        "    w1_indicators_full = normalize_all_features(w1_indicators_full)\n",
        "    \n",
        "    print(h4_indicators_full.columns)\n",
        "    print(d1_indicators_full.columns)\n",
        "    print(w1_indicators_full.columns)\n",
        "\n",
        "    # remove absolute value features\n",
        "    h4_indicators_full = remove_absolute_value_features(h4_indicators_full)\n",
        "    d1_indicators_full = remove_absolute_value_features(d1_indicators_full)\n",
        "    w1_indicators_full = remove_absolute_value_features(w1_indicators_full)\n",
        "\n",
        "    print(h4_indicators_full.columns)\n",
        "    print(d1_indicators_full.columns)\n",
        "    print(w1_indicators_full.columns)   \n",
        "    \n",
        "    print(\"âœ… All indicators calculated on full data\")\n",
        "    return h4_indicators_full, d1_indicators_full, w1_indicators_full\n",
        "\n",
        "# Calculate indicators on full data\n",
        "h4_indicators_full, d1_indicators_full, w1_indicators_full = calculate_indicators_on_full_data(\n",
        "    h4_full, d1_full, w1_full\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "h4_indicators_full.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "h4_used_range=h4_indicators_full[(h4_indicators_full.index>=train_start)& (h4_indicators_full.index<=test_end)] \n",
        "print(h4_used_range.isnull().sum().sum())\n",
        "print(d1_indicators_full[(d1_indicators_full.index>=train_start)& (d1_indicators_full.index<=test_end)].isnull().sum().sum())\n",
        "print(w1_indicators_full[(w1_indicators_full.index>=train_start)& (w1_indicators_full.index<=test_end)].isnull().sum().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import calendar\n",
        "\n",
        "\n",
        "def get_previous_month_timestamp(timestamp):\n",
        "    \"\"\"\n",
        "    Get the 10th day of the previous month\n",
        "    Simple and handles all edge cases!\n",
        "    \"\"\"\n",
        "    dt = pd.to_datetime(timestamp)\n",
        "    \n",
        "    # Get previous month\n",
        "    if dt.month == 1:\n",
        "        prev_month = dt.replace(year=dt.year-1, month=12, day=10)\n",
        "    else:\n",
        "        prev_month = dt.replace(month=dt.month-1, day=10)\n",
        "    \n",
        "    return prev_month\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Step 3.5: Remove Problematic Indicators After Calculation\n",
        "# def remove_problematic_indicators(h4_indicators_full, d1_indicators_full, w1_indicators_full, m1_indicators_full):\n",
        "#     \"\"\"\n",
        "#     Remove indicators that cannot be calculated with available data\n",
        "#     - W1: Remove 120 MA (needs 2.3 years of data)\n",
        "#     - M1: Remove 120 MA, 60 MA, leading_span_A, leading_span_B (need 5-10 years of data)\n",
        "#     \"\"\"\n",
        "#     print(\"ðŸ§¹ Removing problematic indicators...\")\n",
        "    \n",
        "#     # W1: Remove 120 MA\n",
        "#     w1_indicators_clean = w1_indicators_full.copy()\n",
        "#     if 'W1_MA_120' in w1_indicators_clean.columns:\n",
        "#         w1_indicators_clean = w1_indicators_clean.drop('W1_MA_120', axis=1)\n",
        "#         print(\"âœ… Removed W1_MA_120 (needs 2.3 years of data)\")\n",
        "    \n",
        "#     # M1: Remove 120 MA, 60 MA, leading_span_A, leading_span_B\n",
        "#     m1_indicators_clean = m1_indicators_full.copy()\n",
        "#     problematic_m1_cols = ['M1_MA_120', 'M1_MA_60', 'M1_leading_span_A', 'M1_leading_span_B', 'M1_MACD_line', 'M1_MACD_signal', 'M1_MACD_hist']\n",
        "    \n",
        "#     for col in problematic_m1_cols:\n",
        "#         if col in m1_indicators_clean.columns:\n",
        "#             m1_indicators_clean = m1_indicators_clean.drop(col, axis=1)\n",
        "#             print(f\"âœ… Removed {col} (needs 5-10 years of data)\")\n",
        "    \n",
        "#     print(f\"ðŸ“Š Cleaned indicators:\")\n",
        "#     print(f\"  H4: {len(h4_indicators_full.columns)} features (no changes)\")\n",
        "#     print(f\"  D1: {len(d1_indicators_full.columns)} features (no changes)\")\n",
        "#     print(f\"  W1: {len(w1_indicators_clean.columns)} features (removed 1)\")\n",
        "#     print(f\"  M1: {len(m1_indicators_clean.columns)} features (removed 7)\")\n",
        "    \n",
        "#     return h4_indicators_full, d1_indicators_full, w1_indicators_clean, m1_indicators_clean\n",
        "\n",
        "# # Remove problematic indicators\n",
        "# h4_indicators_clean, d1_indicators_clean, w1_indicators_clean = remove_problematic_indicators(\n",
        "#     h4_indicators_full, d1_indicators_full, w1_indicators_full\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Temporal Alignment Functions (CORRECTED VERSION)\n",
        "def align_timeframe_data(base_data, target_data, base_timeframe, target_timeframe):\n",
        "    \"\"\"\n",
        "    Align target timeframe data with base timeframe data using proper temporal alignment\n",
        "    \n",
        "    Args:\n",
        "        base_data: H4 data (base timeframe)\n",
        "        target_data: D1/W1/M1 data (target timeframe)\n",
        "        base_timeframe: 'H4'\n",
        "        target_timeframe: 'D1', 'W1', 'M1', 'D1_lags', 'W1_lags', 'M1_lags'\n",
        "    \n",
        "    Returns:\n",
        "        aligned_data: Target data aligned with base data timestamps\n",
        "    \"\"\"\n",
        "    print(f\"ðŸ”„ Aligning {target_timeframe} data with {base_timeframe} timestamps...\")\n",
        "    \n",
        "    # Define timeframe offsets - ADD LAG SUPPORT\n",
        "    timeframe_offsets = {\n",
        "        'D1': pd.Timedelta(days=1),\n",
        "        'W1': pd.Timedelta(weeks=1),\n",
        "        # Add lag support\n",
        "        'D1_lags': pd.Timedelta(days=1),\n",
        "        'W1_lags': pd.Timedelta(weeks=1)\n",
        "    }\n",
        "    \n",
        "    aligned_data = pd.DataFrame(index=base_data.index, columns=target_data.columns)\n",
        "    \n",
        "    for base_timestamp in base_data.index:\n",
        "        # Use regular timedelta for other timeframes\n",
        "        offset = timeframe_offsets[target_timeframe]\n",
        "        cutoff_time = base_timestamp - offset\n",
        "        \n",
        "        # Find target data that is <= cutoff_time (previous completed data)\n",
        "        available_target_data = target_data[target_data.index <= cutoff_time]\n",
        "        \n",
        "        if len(available_target_data) > 0:\n",
        "            # Use the most recent available data (previous completed)\n",
        "            latest_target_data = available_target_data.iloc[-1]\n",
        "            aligned_data.loc[base_timestamp] = latest_target_data\n",
        "        else:\n",
        "            # If no data available, fill with NaN\n",
        "            aligned_data.loc[base_timestamp] = np.nan\n",
        "    \n",
        "    print(f\"âœ… {target_timeframe} data aligned: {len(aligned_data.columns)} features, {len(aligned_data)} records\")\n",
        "    return aligned_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Create Feature Sets A0â†’A3 with Cleaned Indicators\n",
        "def create_feature_sets_with_cleaned_indicators(h4, d1, w1):\n",
        "    \"\"\"Create feature sets A0â†’A3 using cleaned indicators (no problematic indicators)\"\"\"\n",
        "\n",
        "    h4.columns=[f\"H4_{col}\" for col in h4.columns]\n",
        "    d1.columns=[f\"D1_{col}\" for col in d1.columns]\n",
        "    w1.columns=[f\"W1_{col}\" for col in w1.columns]\n",
        "    \n",
        "    # A0: H4 indicators only\n",
        "    A0 = h4.copy()\n",
        "    \n",
        "    # A1: H4 + D1 indicators - Align D1 with H4 timestamps\n",
        "    d1_aligned = align_timeframe_data(A0, d1, 'H4', 'D1')\n",
        "    A1 = pd.concat([h4, d1_aligned], axis=1)\n",
        "    \n",
        "    # A2: H4 + D1 + W1 indicators - Align W1 with H4 timestamps\n",
        "    w1_aligned = align_timeframe_data(A0, w1, 'H4', 'W1')\n",
        "    A2 = pd.concat([h4, d1_aligned, w1_aligned], axis=1)\n",
        "    \n",
        "    \n",
        "    print(f\"âœ… Feature sets A0â†’A3 created with cleaned indicators:\")\n",
        "    print(f\"  A0: {len(A0.columns)} features, {len(A0)} records\")\n",
        "    print(f\"  A1: {len(A1.columns)} features, {len(A1)} records\")\n",
        "    print(f\"  A2: {len(A2.columns)} features, {len(A2)} records\")\n",
        "    \n",
        "    return A0, A1, A2\n",
        "\n",
        "# Create feature sets with cleaned indicators\n",
        "A0, A1, A2 = create_feature_sets_with_cleaned_indicators(\n",
        "    h4_indicators_full, d1_indicators_full, w1_indicators_full\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate_A0_to_A3(A0, A1, A2):\n",
        "    \"\"\"Validate feature sets A0â†’A3\"\"\"\n",
        "    print(\"ðŸ” Feature Set Validation:\")\n",
        "    train_start = '2020-05-12'\n",
        "    test_end = '2025-09-19'\n",
        "\n",
        "    A0_focused = A0[(A0.index >= train_start) & (A0.index <= test_end)]\n",
        "    A1_focused = A1[(A1.index >= train_start) & (A1.index <= test_end)]\n",
        "    A2_focused = A2[(A2.index >= train_start) & (A2.index <= test_end)]\n",
        "\n",
        "    print(\n",
        "        f\"A0_focused.isnull().sum().sum(): {A0_focused.isnull().sum().sum()}\")\n",
        "    print(\n",
        "        f\"A1_focused.isnull().sum().sum(): {A1_focused.isnull().sum().sum()}\")\n",
        "    print(\n",
        "        f\"A2_focused.isnull().sum().sum(): {A2_focused.isnull().sum().sum()}\")\n",
        "\n",
        "validate_A0_to_A3(A0, A1, A2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 6: Create Historical Lag Features (Using Full Data with Buffer)\n",
        "def create_lag_features(indicators_full, timeframe_name, lag_periods):\n",
        "    \"\"\"Create historical lag features for a timeframe using full data (including buffer)\"\"\"\n",
        "    lag_features = pd.DataFrame(index=indicators_full.index)\n",
        "    \n",
        "    for lag in lag_periods:\n",
        "        for col in indicators_full.columns:\n",
        "            lag_features[f\"{col}_lag_{lag}\"] = indicators_full[col].shift(lag)\n",
        "    \n",
        "    print(f\"âœ… {timeframe_name} lags: {len(lag_features.columns)} features created\")\n",
        "    return lag_features\n",
        "\n",
        "def create_all_lag_features_with_buffer(h4_indicators_clean, d1_indicators_clean, w1_indicators_clean):\n",
        "    \"\"\"Create historical lag features for all timeframes using full data (including buffer)\"\"\"\n",
        "    \n",
        "    print(\"â° Creating historical lag features using full data (including buffer)...\")\n",
        "    \n",
        "    # H4 lags: t-1 to t-6 (6 lags)\n",
        "    h4_lags_full = create_lag_features(h4_indicators_clean, 'H4', range(1, 7))\n",
        "    \n",
        "    # D1 lags: t-1 to t-7 (7 lags)\n",
        "    d1_lags_full = create_lag_features(d1_indicators_clean, 'D1', range(1, 8))\n",
        "    \n",
        "    # W1 lags: t-1 to t-4 (4 lags)\n",
        "    w1_lags_full = create_lag_features(w1_indicators_clean, 'W1', range(1, 5))\n",
        "    \n",
        "    \n",
        "    print(f\"âœ… All lag features created using full data:\")\n",
        "    print(f\"  H4 lags: {len(h4_lags_full.columns)} features, {len(h4_lags_full)} records\")\n",
        "    print(f\"  D1 lags: {len(d1_lags_full.columns)} features, {len(d1_lags_full)} records\")\n",
        "    print(f\"  W1 lags: {len(w1_lags_full.columns)} features, {len(w1_lags_full)} records\")\n",
        "    \n",
        "    return h4_lags_full, d1_lags_full, w1_lags_full\n",
        "\n",
        "# Create historical lag features using full data (including buffer)\n",
        "h4_lags_full, d1_lags_full, w1_lags_full = create_all_lag_features_with_buffer(\n",
        "    A0,A1,A2\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "h4_lags_focused=h4_lags_full[(h4_lags_full.index >= train_start) & (h4_lags_full.index <= test_end)]\n",
        "print(h4_lags_focused.isnull().sum().sum())\n",
        "print(h4_lags_focused.index.min(), h4_lags_focused.index.max())\n",
        "\n",
        "d1_lags_focused = d1_lags_full[(d1_lags_full.index >= train_start)\n",
        "                               & (d1_lags_full.index <= test_end)]\n",
        "print(d1_lags_focused.isnull().sum().sum())\n",
        "print(d1_lags_focused.index.min(), d1_lags_focused.index.max())\n",
        "\n",
        "w1_lags_focused = w1_lags_full[(w1_lags_full.index >= train_start)\n",
        "                              & (w1_lags_full.index <= test_end)]\n",
        "print(w1_lags_focused.isnull().sum().sum())\n",
        "print(w1_lags_focused.index.min(), w1_lags_focused.index.max())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 7: Create A4 Feature Set with Temporal Alignment (Using Full Data)\n",
        "def create_a4_features_with_temporal_alignment(A2:pd.DataFrame, h4_lags_full:pd.DataFrame, d1_lags_full:pd.DataFrame, w1_lags_full:pd.DataFrame):\n",
        "    \"\"\"Create A4 feature set: A3 + all historical lags with proper temporal alignment\"\"\"\n",
        "    \n",
        "    # Align D1, W1, M1 lag features with H4 timestamps\n",
        "    d1_lags_aligned = align_timeframe_data(A2, d1_lags_full, 'H4', 'D1_lags')\n",
        "    w1_lags_aligned = align_timeframe_data(A2, w1_lags_full, 'H4', 'W1_lags')\n",
        "    \n",
        "    # Combine A3 with all lag features\n",
        "    h4_lags_full_aligned = h4_lags_full.copy()\n",
        "    h4_lags_full_aligned.columns = [f\"H4_lags_{col}\" for col in h4_lags_full.columns]\n",
        "    d1_lags_aligned.columns = [f\"D1_lags_{col}\" for col in d1_lags_aligned.columns]\n",
        "    w1_lags_aligned.columns = [f\"W1_lags_{col}\" for col in w1_lags_aligned.columns]\n",
        "    A3 = pd.concat([A2, h4_lags_full_aligned, d1_lags_aligned, w1_lags_aligned], axis=1)\n",
        "    \n",
        "    print(f\"âœ… A4 feature set created with temporal alignment:\")\n",
        "    print(f\"  A4: {len(A3.columns)} features, {len(A3)} records\")\n",
        "    print(f\"  - Current indicators: {len(A2.columns)}\")\n",
        "    print(f\"  - Historical lags: {len(A3.columns) - len(A2.columns)}\")\n",
        "    \n",
        "    return A3\n",
        "\n",
        "# Create A4 feature set with temporal alignment\n",
        "A3 = create_a4_features_with_temporal_alignment(A2, h4_lags_full, d1_lags_full, w1_lags_full)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
        "    print(A3.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 8: Data Validation & Quality Checks\n",
        "def validate_feature_sets(A0,\n",
        "                          A1,\n",
        "                          A2,\n",
        "                          A3,\n",
        "                          train_start='2020-05-12',\n",
        "                          test_end='2025-09-19'):\n",
        "    \"\"\"Validate all feature sets\"\"\"\n",
        "    print(\"ðŸ” Feature Set Validation (Train/Test Period Only):\")\n",
        "    print(f\"ðŸ“… Period: {train_start} to {test_end}\")\n",
        "\n",
        "    A0_focused = A0[(A0.index >= train_start) & (A0.index <= test_end)]\n",
        "    A1_focused = A1[(A1.index >= train_start) & (A1.index <= test_end)]\n",
        "    A2_focused = A2[(A2.index >= train_start) & (A2.index <= test_end)]\n",
        "    A3_focused = A3[(A3.index >= train_start) & (A3.index <= test_end)]\n",
        "\n",
        "    feature_counts = {\n",
        "        'A0': len(A0_focused.columns),\n",
        "        'A1': len(A1_focused.columns),\n",
        "        'A2': len(A2_focused.columns),\n",
        "        'A3': len(A3_focused.columns)\n",
        "    }\n",
        "\n",
        "    # A0 : 19, A1 : 38, A2 : 38 + 19 - 1= 56,  A3 : 56 + 19 - 4 = 71, A4 : \n",
        "    expected_counts = {'A0': 19, 'A1': 38, 'A2': 56, 'A3': 68}\n",
        "\n",
        "    print(\"ðŸ” Feature Set Validation:\")\n",
        "    for set_name, count in feature_counts.items():\n",
        "        expected = expected_counts[set_name]\n",
        "        status = \"âœ…\" if count == expected else \"âŒ\"\n",
        "        print(f\"  {status} {set_name}: {count}/{expected} features\")\n",
        "\n",
        "    # Check for missing values in focused period only\n",
        "    print(\"\\nðŸ” Missing Values Check (Train/Test Period Only):\")\n",
        "    for set_name, features in [('A0', A0_focused), ('A1', A1_focused),\n",
        "                               ('A2', A2_focused), ('A3', A3_focused)]:\n",
        "        missing_count = features.isnull().sum().sum()\n",
        "        total_cells = features.shape[0] * features.shape[1]\n",
        "        missing_percentage = (missing_count / total_cells) * 100\n",
        "        print(\n",
        "            f\"  {set_name}: {missing_count:,} missing values ({missing_percentage:.2f}%)\"\n",
        "        )\n",
        "        print(f\"    Period: {features.index[0]} to {features.index[-1]}\")\n",
        "        print(f\"    Records: {len(features)}\")\n",
        "\n",
        "    return feature_counts, (A0_focused, A1_focused, A2_focused, A3_focused)\n",
        "\n",
        "# Validate feature sets\n",
        "validation_results_focused, focused_sets = validate_feature_sets(A0, A1, A2, A3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "*_,A3_focused=focused_sets\n",
        "\n",
        "# Check if M1 alignment is working correctly\n",
        "print(\"M1 temporal alignment check:\")\n",
        "print(f\"A3 missing values: {A3_focused.isnull().sum().sum()}\")\n",
        "print(\n",
        "    f\"A3 M1 columns missing: {A3_focused.filter(regex='M1_').isnull().sum().sum()}\"\n",
        ")\n",
        "\n",
        "# Check lag feature missing values\n",
        "print(\"Lag feature missing values:\")\n",
        "print(f\"A4 H4 lags missing: {A3_focused.filter(regex='H4_.*_lag_').isnull().sum().sum()}\")\n",
        "print(f\"A4 D1 lags missing: {A3_focused.filter(regex='D1_.*_lag_').isnull().sum().sum()}\")\n",
        "print(f\"A4 W1 lags missing: {A3_focused.filter(regex='W1_.*_lag_').isnull().sum().sum()}\")\n",
        "print(f\"A4 M1 lags missing: {A3_focused.filter(regex='M1_.*_lag_').isnull().sum().sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "duplicated_cols = A3.columns[A3.columns.duplicated()]\n",
        "print(f\"\\nì¤‘ë³µëœ ì»¬ëŸ¼ëª…: {duplicated_cols.tolist()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def create_target_variable_first_threshold(h4_full: pd.DataFrame,\n",
        "                                                window_size: int = 30, # <-- 30ë´‰ìœ¼ë¡œ ë³€ê²½\n",
        "                                                upper_threshold: float = 0.10, # +10%\n",
        "                                                lower_threshold: float = -0.15, # -15%\n",
        "                                                train_start: str = '2020-05-12',\n",
        "                                                test_end: str = '2025-09-19') -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Create target variable using first threshold logic (slow loop version).\n",
        "    y=0: upper_threshold hit first OR neither hit.\n",
        "    y=1: lower_threshold hit first.\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"ðŸŽ¯ Creating target variable ({window_size}-period window, first threshold logic)...\")\n",
        "\n",
        "    # Create target variable for ALL H4 data\n",
        "    y_full = pd.DataFrame(index=h4_full.index)\n",
        "    y_full['target'] = 0  # Initialize with 0 (REST/BUY state)\n",
        "\n",
        "    print(\"ðŸ”„ Calculating target labels (this might take a while)...\")\n",
        "\n",
        "    total_len = len(h4_full)\n",
        "    for i in range(total_len):\n",
        "        # Print progress\n",
        "        if i % 1000 == 0:\n",
        "            print(f\"   Processing {i}/{total_len} records...\")\n",
        "\n",
        "        # Ensure there's enough future data for the window\n",
        "        if i + window_size >= total_len:\n",
        "            # Not enough future data, keep default y=0 and continue to next i\n",
        "            continue\n",
        "\n",
        "        current_close = h4_full.iloc[i]['close']\n",
        "        \n",
        "        # Avoid division by zero if current_close is 0\n",
        "        if current_close == 0:\n",
        "            continue # Keep default y=0\n",
        "\n",
        "        # Get the next `window_size` periods\n",
        "        future_data = h4_full.iloc[i + 1 : i + 1 + window_size] # <-- 180 ëŒ€ì‹  window_size ì‚¬ìš©\n",
        "\n",
        "        # Calculate price changes relative to current close\n",
        "        price_increases = (future_data['close'] - current_close) / current_close\n",
        "        price_drops = (future_data['low'] - current_close) / current_close # Check drops against future 'low'\n",
        "\n",
        "        # Find the first threshold hit within the window\n",
        "        target_set = False # Flag to check if target was set inside the inner loop\n",
        "        for j in range(len(future_data)):\n",
        "            # Check lower threshold (-15%)\n",
        "            if price_drops.iloc[j] <= lower_threshold:\n",
        "                y_full.iloc[i, 0] = 1  # Lower hit first -> SELL\n",
        "                target_set = True\n",
        "                break # Exit inner loop once a threshold is hit\n",
        "            \n",
        "            # Check upper threshold (+10%)\n",
        "            if price_increases.iloc[j] >= upper_threshold:\n",
        "                y_full.iloc[i, 0] = 0  # Upper hit first -> REST/BUY\n",
        "                target_set = True\n",
        "                break # Exit inner loop once a threshold is hit\n",
        "\n",
        "        # If the inner loop finished without hitting either threshold (no break)\n",
        "        # the default y=0 (initialized at the beginning) remains.\n",
        "\n",
        "    # --- í•„í„°ë§ ìœ„ì¹˜ ìˆ˜ì • ---\n",
        "    # The main loop is finished, now filter the results\n",
        "    print(\"Filtering results to the specified date range...\")\n",
        "    y_focused = y_full[(y_full.index >= train_start)\n",
        "                       & (y_full.index <= test_end)].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
        "\n",
        "    print(f\"âœ… Target variable created:\")\n",
        "    print(f\"   Total records in focused period: {len(y_focused)}\")\n",
        "    print(f\"   Sell labels (target=1): {y_focused['target'].sum()}\")\n",
        "    print(f\"   Rest/Buy labels (target=0): {len(y_focused) - y_focused['target'].sum()}\")\n",
        "\n",
        "    return y_focused\n",
        "\n",
        "# --- í•¨ìˆ˜ í˜¸ì¶œ ì˜ˆì‹œ ---\n",
        "# y_target = create_target_variable_first_threshold_slow(h4_full)\n",
        "# print(y_target.head())\n",
        "# print(y_target['target'].value_counts())\n",
        "\n",
        "# create_target_variable_first_threshold(h4_full)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_focused_feature_sets_complete(A0, A1, A2, A3, h4_full, train_start='2020-05-12', test_end='2025-09-19'):\n",
        "    \"\"\"Save focused feature sets with correct target variable creation\"\"\"\n",
        "    \n",
        "    print(\"ðŸ’¾ Saving focused feature sets...\")\n",
        "    \n",
        "    # Create features directory\n",
        "    features_dir = Path('../features')\n",
        "    features_dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    # Filter feature sets to focused period\n",
        "    def filter_focused_period(data, start_date, end_date):\n",
        "        return data[(data.index >= start_date) & (data.index <= end_date)]\n",
        "    \n",
        "    # Save feature sets\n",
        "    A0_focused = filter_focused_period(A0, train_start, test_end)\n",
        "    A1_focused = filter_focused_period(A1, train_start, test_end)\n",
        "    A2_focused = filter_focused_period(A2, train_start, test_end)\n",
        "    A3_focused = filter_focused_period(A3, train_start, test_end)\n",
        "    \n",
        "    # Create target variable using FULL H4 data\n",
        "    y_focused = create_target_variable_first_threshold(h4_full,train_start=train_start, test_end=test_end)\n",
        "    \n",
        "    # Save all files\n",
        "    A0_focused.to_parquet(features_dir / 'A0.parquet')\n",
        "    A1_focused.to_parquet(features_dir / 'A1.parquet')\n",
        "    A2_focused.to_parquet(features_dir / 'A2.parquet')\n",
        "    A3_focused.to_parquet(features_dir / 'A3.parquet')\n",
        "    y_focused.to_parquet(features_dir / 'y.parquet')\n",
        "    \n",
        "    print(\"ðŸŽ‰ All feature sets saved successfully!\")\n",
        "    return A0_focused, A1_focused, A2_focused, A3_focused, y_focused\n",
        "\n",
        "# Run the complete save function\n",
        "save_focused_feature_sets_complete(A0, A1, A2, A3,  h4_full, train_start='2020-05-12', test_end='2025-09-19')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate_saved_feature_sets(features_dir='../features'):\n",
        "    \"\"\"\n",
        "    Validate that all feature sets and target variable were saved correctly\n",
        "    \n",
        "    Args:\n",
        "        features_dir: Path to features directory\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"ðŸ” Validating Saved Feature Sets...\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Check if features directory exists\n",
        "    features_path = Path(features_dir)\n",
        "    if not features_path.exists():\n",
        "        print(\"âŒ Features directory not found!\")\n",
        "        return\n",
        "    \n",
        "    # Expected files\n",
        "    expected_files = ['A0.parquet', 'A1.parquet', 'A2.parquet', 'A3.parquet', 'A4.parquet', 'y.parquet']\n",
        "    \n",
        "    print(\"ðŸ“ File Existence Check:\")\n",
        "    for file in expected_files:\n",
        "        file_path = features_path / file\n",
        "        if file_path.exists():\n",
        "            print(f\"  âœ… {file} - Found\")\n",
        "        else:\n",
        "            print(f\"  âŒ {file} - Missing!\")\n",
        "    \n",
        "    print(\"\\nðŸ“Š Data Validation:\")\n",
        "    \n",
        "    # Load and validate each file\n",
        "    for file in expected_files:\n",
        "        file_path = features_path / file\n",
        "        if not file_path.exists():\n",
        "            continue\n",
        "            \n",
        "        print(f\"\\nðŸ” Validating {file}:\")\n",
        "        \n",
        "        try:\n",
        "            # Load data\n",
        "            data = pd.read_parquet(file_path)\n",
        "            \n",
        "            # Basic info\n",
        "            print(f\"  ðŸ“ Shape: {data.shape[0]} records Ã— {data.shape[1]} features\")\n",
        "            print(f\"  ðŸ“… Period: {data.index[0]} to {data.index[-1]}\")\n",
        "            \n",
        "            # Check for missing values\n",
        "            missing_count = data.isnull().sum().sum()\n",
        "            total_cells = data.shape[0] * data.shape[1]\n",
        "            missing_percentage = (missing_count / total_cells) * 100 if total_cells > 0 else 0\n",
        "            \n",
        "            if missing_count == 0:\n",
        "                print(f\"  âœ… Missing values: {missing_count} (0.00%)\")\n",
        "            else:\n",
        "                print(f\"  âš ï¸ Missing values: {missing_count} ({missing_percentage:.2f}%)\")\n",
        "                \n",
        "                # Show which columns have missing values\n",
        "                missing_cols = data.isnull().sum()\n",
        "                missing_cols = missing_cols[missing_cols > 0]\n",
        "                if len(missing_cols) > 0:\n",
        "                    print(f\"    Columns with missing values:\")\n",
        "                    for col, count in missing_cols.items():\n",
        "                        print(f\"      {col}: {count} missing\")\n",
        "            \n",
        "            # Check data types\n",
        "            print(f\"  ðŸ“‹ Data types: {data.dtypes.value_counts().to_dict()}\")\n",
        "            \n",
        "            # Check for infinite values\n",
        "            inf_count = np.isinf(data.select_dtypes(include=[np.number])).sum().sum()\n",
        "            if inf_count == 0:\n",
        "                print(f\"  âœ… Infinite values: {inf_count}\")\n",
        "            else:\n",
        "                print(f\"  âš ï¸ Infinite values: {inf_count}\")\n",
        "            \n",
        "            # Specific validation for target variable\n",
        "            if file == 'y.parquet':\n",
        "                print(f\"  ðŸŽ¯ Target variable validation:\")\n",
        "                print(f\"    Unique values: {data['target'].unique()}\")\n",
        "                print(f\"    Value counts: {data['target'].value_counts().to_dict()}\")\n",
        "                print(f\"    Sell percentage: {data['target'].mean()*100:.2f}%\")\n",
        "            \n",
        "            # Specific validation for feature sets\n",
        "            if file.startswith('A'):\n",
        "                print(f\"  ðŸ”¢ Feature set validation:\")\n",
        "                print(f\"    Feature count: {len(data.columns)}\")\n",
        "                print(f\"    Sample features: {list(data.columns[:5])}\")\n",
        "                \n",
        "                # Check for expected feature counts\n",
        "                expected_counts = {'A0': 19, 'A1': 38, 'A2': 56, 'A3': 67, 'A4': 416}\n",
        "                if file.replace('.parquet', '') in expected_counts:\n",
        "                    expected = expected_counts[file.replace('.parquet', '')]\n",
        "                    actual = len(data.columns)\n",
        "                    if actual == expected:\n",
        "                        print(f\"    âœ… Feature count matches expected: {actual}\")\n",
        "                    else:\n",
        "                        print(f\"    âš ï¸ Feature count mismatch: {actual} (expected {expected})\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  âŒ Error loading {file}: {e}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"ðŸŽ‰ Validation Complete!\")\n",
        "\n",
        "# Run validation\n",
        "validate_saved_feature_sets('../features')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ” Loading target variable for analysis...\n",
            "ðŸŽ¯ BTC Sell Signal Target Distribution\n",
            "============================================================\n",
            "ðŸ“Š Basic Statistics:\n",
            "  Total samples: 11,737\n",
            "  SELL (target=1): 1,560 (13.29%)\n",
            "  REST (target=0): 10,177 (86.71%)\n",
            "  Class imbalance ratio: 6.52:1 (REST:SELL)\n",
            "\n",
            "ðŸ“ˆ Value Counts:\n",
            "  REST (target=0): 10,177 (86.71%)\n",
            "  SELL (target=1): 1,560 (13.29%)\n",
            "\n",
            "ðŸ“… Temporal Distribution:\n",
            "  Yearly Distribution:\n",
            "    2020: 77 SELL / 1327 REST (5.48% SELL)\n",
            "    2021: 637 SELL / 1553 REST (29.09% SELL)\n",
            "    2022: 464 SELL / 1726 REST (21.19% SELL)\n",
            "    2023: 90 SELL / 2100 REST (4.11% SELL)\n",
            "    2024: 198 SELL / 1998 REST (9.02% SELL)\n",
            "    2025: 94 SELL / 1473 REST (6.00% SELL)\n",
            "\n",
            "  Monthly Distribution (Last 12 months):\n",
            "    2024-10: 0 SELL / 186 REST (0.00% SELL)\n",
            "    2024-11: 0 SELL / 180 REST (0.00% SELL)\n",
            "    2024-12: 12 SELL / 174 REST (6.45% SELL)\n",
            "    2025-01: 23 SELL / 163 REST (12.37% SELL)\n",
            "    2025-02: 43 SELL / 125 REST (25.60% SELL)\n",
            "    2025-03: 23 SELL / 163 REST (12.37% SELL)\n",
            "    2025-04: 5 SELL / 175 REST (2.78% SELL)\n",
            "    2025-05: 0 SELL / 186 REST (0.00% SELL)\n",
            "    2025-06: 0 SELL / 180 REST (0.00% SELL)\n",
            "    2025-07: 0 SELL / 186 REST (0.00% SELL)\n",
            "    2025-08: 0 SELL / 186 REST (0.00% SELL)\n",
            "    2025-09: 0 SELL / 109 REST (0.00% SELL)\n",
            "\n",
            "ðŸ” Consecutive SELL Analysis:\n",
            "  Max consecutive SELLs: 87\n",
            "  Average consecutive SELLs: 15.00\n",
            "  Total consecutive SELL sequences: 104\n",
            "\n",
            "ðŸŽ¯ SELL Clustering Analysis:\n",
            "  Average time between SELLs: 27.4 hours\n",
            "  Min time between SELLs: 4.0 hours\n",
            "  Max time between SELLs: 3516.0 hours\n",
            "  SELLs within 20 hours: 1491 (95.6%)\n",
            "\n",
            "ðŸ“Š Target Distribution Summary:\n",
            "  This is a highly imbalanced dataset with 13.29% SELL signals\n",
            "  Class imbalance ratio: 6.5:1\n",
            "  This imbalance will require careful handling in model training\n",
            "  Consider using techniques like:\n",
            "    - Class weights (scale_pos_weight)\n",
            "    - SMOTE or other oversampling\n",
            "    - Focal Loss\n",
            "    - Stratified sampling\n"
          ]
        }
      ],
      "source": [
        "# Target Variable Distribution Analysis\n",
        "def analyze_target_distribution(y_data, title=\"Target Variable Distribution\"):\n",
        "    \"\"\"\n",
        "    Analyze and visualize target variable distribution\n",
        "    \n",
        "    Args:\n",
        "        y_data: DataFrame with 'target' column\n",
        "        title: Title for the analysis\n",
        "    \"\"\"\n",
        "    print(f\"ðŸŽ¯ {title}\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Basic statistics\n",
        "    total_samples = len(y_data)\n",
        "    sell_samples = y_data['target'].sum()\n",
        "    rest_samples = total_samples - sell_samples\n",
        "    sell_percentage = (sell_samples / total_samples) * 100\n",
        "    \n",
        "    print(f\"ðŸ“Š Basic Statistics:\")\n",
        "    print(f\"  Total samples: {total_samples:,}\")\n",
        "    print(f\"  SELL (target=1): {sell_samples:,} ({sell_percentage:.2f}%)\")\n",
        "    print(f\"  REST (target=0): {rest_samples:,} ({100-sell_percentage:.2f}%)\")\n",
        "    \n",
        "    # Class imbalance ratio\n",
        "    imbalance_ratio = rest_samples / sell_samples\n",
        "    print(f\"  Class imbalance ratio: {imbalance_ratio:.2f}:1 (REST:SELL)\")\n",
        "    \n",
        "    # Value counts\n",
        "    print(f\"\\nðŸ“ˆ Value Counts:\")\n",
        "    value_counts = y_data['target'].value_counts().sort_index()\n",
        "    for value, count in value_counts.items():\n",
        "        label = \"SELL\" if value == 1 else \"REST\"\n",
        "        percentage = (count / total_samples) * 100\n",
        "        print(f\"  {label} (target={value}): {count:,} ({percentage:.2f}%)\")\n",
        "    \n",
        "    # Temporal distribution analysis\n",
        "    print(f\"\\nðŸ“… Temporal Distribution:\")\n",
        "    y_data_with_date = y_data.copy()\n",
        "    y_data_with_date['year'] = y_data_with_date.index.year\n",
        "    y_data_with_date['month'] = y_data_with_date.index.month\n",
        "    y_data_with_date['year_month'] = y_data_with_date.index.to_period('M')\n",
        "    \n",
        "    # Yearly distribution\n",
        "    yearly_dist = y_data_with_date.groupby('year')['target'].agg(['count', 'sum']).reset_index()\n",
        "    yearly_dist['sell_pct'] = (yearly_dist['sum'] / yearly_dist['count']) * 100\n",
        "    yearly_dist['rest_count'] = yearly_dist['count'] - yearly_dist['sum']\n",
        "    \n",
        "    print(f\"  Yearly Distribution:\")\n",
        "    for _, row in yearly_dist.iterrows():\n",
        "        print(f\"    {int(row['year'])}: {int(row['sum'])} SELL / {int(row['rest_count'])} REST ({row['sell_pct']:.2f}% SELL)\")\n",
        "    \n",
        "    # Monthly distribution (last 12 months)\n",
        "    monthly_dist = y_data_with_date.groupby('year_month')['target'].agg(['count', 'sum']).reset_index()\n",
        "    monthly_dist['sell_pct'] = (monthly_dist['sum'] / monthly_dist['count']) * 100\n",
        "    monthly_dist['rest_count'] = monthly_dist['count'] - monthly_dist['sum']\n",
        "    \n",
        "    print(f\"\\n  Monthly Distribution (Last 12 months):\")\n",
        "    last_12_months = monthly_dist.tail(12)\n",
        "    for _, row in last_12_months.iterrows():\n",
        "        print(f\"    {row['year_month']}: {int(row['sum'])} SELL / {int(row['rest_count'])} REST ({row['sell_pct']:.2f}% SELL)\")\n",
        "    \n",
        "    # Consecutive SELL analysis\n",
        "    print(f\"\\nðŸ” Consecutive SELL Analysis:\")\n",
        "    y_series = y_data['target'].values\n",
        "    consecutive_sells = []\n",
        "    current_consecutive = 0\n",
        "    \n",
        "    for i, value in enumerate(y_series):\n",
        "        if value == 1:  # SELL\n",
        "            current_consecutive += 1\n",
        "        else:  # REST\n",
        "            if current_consecutive > 0:\n",
        "                consecutive_sells.append(current_consecutive)\n",
        "                current_consecutive = 0\n",
        "    \n",
        "    if current_consecutive > 0:  # Handle case where series ends with SELL\n",
        "        consecutive_sells.append(current_consecutive)\n",
        "    \n",
        "    if consecutive_sells:\n",
        "        max_consecutive = max(consecutive_sells)\n",
        "        avg_consecutive = sum(consecutive_sells) / len(consecutive_sells)\n",
        "        print(f\"  Max consecutive SELLs: {max_consecutive}\")\n",
        "        print(f\"  Average consecutive SELLs: {avg_consecutive:.2f}\")\n",
        "        print(f\"  Total consecutive SELL sequences: {len(consecutive_sells)}\")\n",
        "    else:\n",
        "        print(f\"  No consecutive SELL sequences found\")\n",
        "    \n",
        "    # SELL clustering analysis (SELLs within 5 periods)\n",
        "    print(f\"\\nðŸŽ¯ SELL Clustering Analysis:\")\n",
        "    sell_indices = y_data[y_data['target'] == 1].index\n",
        "    if len(sell_indices) > 1:\n",
        "        time_diffs = []\n",
        "        for i in range(1, len(sell_indices)):\n",
        "            diff = (sell_indices[i] - sell_indices[i-1]).total_seconds() / 3600  # hours\n",
        "            time_diffs.append(diff)\n",
        "        \n",
        "        if time_diffs:\n",
        "            avg_time_between_sells = sum(time_diffs) / len(time_diffs)\n",
        "            min_time_between_sells = min(time_diffs)\n",
        "            max_time_between_sells = max(time_diffs)\n",
        "            \n",
        "            print(f\"  Average time between SELLs: {avg_time_between_sells:.1f} hours\")\n",
        "            print(f\"  Min time between SELLs: {min_time_between_sells:.1f} hours\")\n",
        "            print(f\"  Max time between SELLs: {max_time_between_sells:.1f} hours\")\n",
        "            \n",
        "            # Count SELLs within 5 periods (20 hours for H4)\n",
        "            close_sells = sum(1 for diff in time_diffs if diff <= 20)\n",
        "            print(f\"  SELLs within 20 hours: {close_sells} ({close_sells/len(time_diffs)*100:.1f}%)\")\n",
        "    \n",
        "    return {\n",
        "        'total_samples': total_samples,\n",
        "        'sell_samples': sell_samples,\n",
        "        'rest_samples': rest_samples,\n",
        "        'sell_percentage': sell_percentage,\n",
        "        'imbalance_ratio': imbalance_ratio,\n",
        "        'yearly_distribution': yearly_dist,\n",
        "        'monthly_distribution': monthly_dist,\n",
        "        'consecutive_sells': consecutive_sells\n",
        "    }\n",
        "\n",
        "# Load and analyze target distribution\n",
        "print(\"ðŸ” Loading target variable for analysis...\")\n",
        "y_analysis = pd.read_parquet('../features/y.parquet')\n",
        "\n",
        "# Analyze target distribution\n",
        "target_stats = analyze_target_distribution(y_analysis, \"BTC Sell Signal Target Distribution\")\n",
        "\n",
        "# Additional visualization\n",
        "print(f\"\\nðŸ“Š Target Distribution Summary:\")\n",
        "print(f\"  This is a highly imbalanced dataset with {target_stats['sell_percentage']:.2f}% SELL signals\")\n",
        "print(f\"  Class imbalance ratio: {target_stats['imbalance_ratio']:.1f}:1\")\n",
        "print(f\"  This imbalance will require careful handling in model training\")\n",
        "print(f\"  Consider using techniques like:\")\n",
        "print(f\"    - Class weights (scale_pos_weight)\")\n",
        "print(f\"    - SMOTE or other oversampling\")\n",
        "print(f\"    - Focal Loss\")\n",
        "print(f\"    - Stratified sampling\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2.2 Implementation Complete! âœ…\n",
        "\n",
        "### **Summary of CORRECTED Implementation**\n",
        "\n",
        "**Approach**: Calculate everything together, split on save step.\n",
        "\n",
        "**Steps Completed**:\n",
        "1. âœ… **Load Full Data**: Complete dataset including buffer (2020-03-01 to 2025-10-19)\n",
        "2. âœ… **Technical Indicators**: Calculated all 19 indicators per timeframe on full data\n",
        "3. âœ… **Feature Sets A0â†’A3**: Created incremental feature sets with proper temporal alignment\n",
        "4. âœ… **Historical Lag Features**: Created lag features using full data for complete historical context\n",
        "5. âœ… **A4 Feature Set**: Combined A3 + all historical lags (437 features)\n",
        "6. âœ… **Data Validation**: Verified feature counts and missing values\n",
        "7. âœ… **Save Clean Data**: Split clean period (2020-05-12 to 2025-09-19) only at save step\n",
        "\n",
        "### **Key Fixes Applied**:\n",
        "1. **Full Data Calculations**: All indicators and lags calculated on complete dataset\n",
        "2. **Proper Temporal Alignment**: Enhanced alignment logic with timeframe-specific offsets\n",
        "3. **Complete Historical Context**: W1 data from 2020-05-04, M1 data from 2020-05-01\n",
        "4. **No Missing Data**: Full historical context for all lag features\n",
        "5. **Clean Final Output**: Buffer data used for calculations but not stored\n",
        "\n",
        "### **Temporal Alignment Logic**:\n",
        "- **H4 timestamp 2020-05-11 00:00:00** (candle closed at 2020-05-11 04:00:00):\n",
        "  - **D1 data**: `base_timestamp - 1d` â†’ Use 2020-05-10 00:00:00 (previous day's close) âœ…\n",
        "  - **W1 data**: `base_timestamp - 1w` â†’ Use 2020-05-04 00:00:00 (previous week's close) âœ…\n",
        "  - **M1 data**: `base_timestamp - 1m` â†’ Use 2020-04-11 00:00:00 (previous month's close) âœ…\n",
        "- **Uses timeframe-specific offsets** to ensure proper temporal alignment\n",
        "- **Ensures no future data leakage** and realistic trading scenarios\n",
        "\n",
        "### **Expected Outputs**\n",
        "- **A0.parquet**: 19 features (H4 only)\n",
        "- **A1.parquet**: 38 features (H4 + D1)\n",
        "- **A2.parquet**: 57 features (H4 + D1 + W1)\n",
        "- **A3.parquet**: 76 features (H4 + D1 + W1 + M1)\n",
        "- **A4.parquet**: 437 features (A3 + all historical lags)\n",
        "\n",
        "### **Next Steps**\n",
        "- **Step 3**: Train/test split based on timeline\n",
        "- **Step 4**: Ablation Study experiments (A0â†’A4_Pruned)\n",
        "- **Step 5**: Results analysis and RQ answers\n",
        "\n",
        "**Ready to proceed to Step 3!** ðŸš€\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš€ Testing specific combinations...\n",
            "ðŸ§ª Testing Specific Combinations\n",
            "============================================================\n",
            "\n",
            "ðŸ§ª Window=40, Upper=+10.0%, Lower=-10.0% (40w +10/-10)\n",
            "------------------------------------------------------------\n",
            "ðŸŽ¯ Creating target variable (40-period window, first threshold logic)...\n",
            "ðŸ”„ Calculating target labels (this might take a while)...\n",
            "   Processing 0/12403 records...\n",
            "   Processing 1000/12403 records...\n",
            "   Processing 2000/12403 records...\n",
            "   Processing 3000/12403 records...\n",
            "   Processing 4000/12403 records...\n",
            "   Processing 5000/12403 records...\n",
            "   Processing 6000/12403 records...\n",
            "   Processing 7000/12403 records...\n",
            "   Processing 8000/12403 records...\n",
            "   Processing 9000/12403 records...\n",
            "   Processing 10000/12403 records...\n",
            "   Processing 11000/12403 records...\n",
            "   Processing 12000/12403 records...\n",
            "Filtering results to the specified date range...\n",
            "âœ… Target variable created:\n",
            "   Total records in focused period: 11737\n",
            "   Sell labels (target=1): 1763\n",
            "   Rest/Buy labels (target=0): 9974\n",
            "   ðŸ“Š Overall SELL: 15.02% (1,763/11,737)\n",
            "   ðŸ“… Monthly consistency: 64.6% (42/65 months)\n",
            "   Avg monthly SELL: 14.96% | Std: 16.34%\n",
            "\n",
            "ðŸ§ª Window=50, Upper=+10.0%, Lower=-12.0% (50w +10/-12)\n",
            "------------------------------------------------------------\n",
            "ðŸŽ¯ Creating target variable (50-period window, first threshold logic)...\n",
            "ðŸ”„ Calculating target labels (this might take a while)...\n",
            "   Processing 0/12403 records...\n",
            "   Processing 1000/12403 records...\n",
            "   Processing 2000/12403 records...\n",
            "   Processing 3000/12403 records...\n",
            "   Processing 4000/12403 records...\n",
            "   Processing 5000/12403 records...\n",
            "   Processing 6000/12403 records...\n",
            "   Processing 7000/12403 records...\n",
            "   Processing 8000/12403 records...\n",
            "   Processing 9000/12403 records...\n",
            "   Processing 10000/12403 records...\n",
            "   Processing 11000/12403 records...\n",
            "   Processing 12000/12403 records...\n",
            "Filtering results to the specified date range...\n",
            "âœ… Target variable created:\n",
            "   Total records in focused period: 11737\n",
            "   Sell labels (target=1): 1560\n",
            "   Rest/Buy labels (target=0): 10177\n",
            "   ðŸ“Š Overall SELL: 13.29% (1,560/11,737)\n",
            "   ðŸ“… Monthly consistency: 63.1% (41/65 months)\n",
            "   Avg monthly SELL: 13.20% | Std: 16.59%\n",
            "\n",
            "ðŸ“Š Summary (Specific Combinations)\n",
            "================================================================================\n",
            "Config          Overall%   MonthlyAvg%  Consistency% Std%    \n",
            "--------------------------------------------------------------------------------\n",
            "40w +10/-10        15.02%      14.96%       64.6%  16.34%\n",
            "50w +10/-12        13.29%      13.20%       63.1%  16.59%\n"
          ]
        }
      ],
      "source": [
        "# íŠ¹ì • ì¡°í•© í…ŒìŠ¤íŠ¸: Window 40 (+10/-10), Window 50 (+10/-12)\n",
        "\n",
        "def test_specific_combinations(h4_full: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    ì‚¬ìš©ìžê°€ ìš”ì²­í•œ ë‘ ì¡°í•©ë§Œ í…ŒìŠ¤íŠ¸\n",
        "    - Window: 40, Upper: +10.0%, Lower: -10.0%\n",
        "    - Window: 50, Upper: +10.0%, Lower: -12.0%\n",
        "    \"\"\"\n",
        "    print(\"ðŸ§ª Testing Specific Combinations\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    test_configs = [\n",
        "        {\"window\": 40, \"upper\": 0.10, \"lower\": -0.10, \"name\": \"40w +10/-10\"},\n",
        "        {\"window\": 50, \"upper\": 0.10, \"lower\": -0.12, \"name\": \"50w +10/-12\"},\n",
        "    ]\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    for config in test_configs:\n",
        "        print(f\"\\nðŸ§ª Window={config['window']}, Upper=+{config['upper']*100:.1f}%, Lower={config['lower']*100:.1f}% ({config['name']})\")\n",
        "        print(\"-\" * 60)\n",
        "        \n",
        "        try:\n",
        "            y_result = create_target_variable_first_threshold(\n",
        "                h4_full,\n",
        "                window_size=config['window'],\n",
        "                upper_threshold=config['upper'],\n",
        "                lower_threshold=config['lower'],\n",
        "                train_start='2020-05-12',\n",
        "                test_end='2025-09-19'\n",
        "            )\n",
        "            \n",
        "            # Monthly aggregation\n",
        "            monthly = {}\n",
        "            for idx, row in y_result.iterrows():\n",
        "                ym = idx.to_period('M')\n",
        "                if ym not in monthly:\n",
        "                    monthly[ym] = {\"total\": 0, \"sell\": 0}\n",
        "                monthly[ym][\"total\"] += 1\n",
        "                monthly[ym][\"sell\"] += int(row[\"target\"] == 1)\n",
        "            \n",
        "            monthly_rows = []\n",
        "            for ym in sorted(monthly.keys()):\n",
        "                total = monthly[ym][\"total\"]\n",
        "                sell = monthly[ym][\"sell\"]\n",
        "                sell_ratio = sell / total * 100 if total else 0\n",
        "                monthly_rows.append({\"ym\": ym, \"total\": total, \"sell\": sell, \"sell_ratio\": sell_ratio})\n",
        "            \n",
        "            # Summary\n",
        "            total_samples = len(y_result)\n",
        "            total_sell = int(y_result[\"target\"].sum())\n",
        "            overall_ratio = total_sell / total_samples * 100 if total_samples else 0\n",
        "            valid_months = [r for r in monthly_rows if r[\"total\"] > 0]\n",
        "            months_with_sell = sum(1 for r in valid_months if r[\"sell_ratio\"] > 0)\n",
        "            consistency = months_with_sell / len(valid_months) * 100 if valid_months else 0\n",
        "            ratios = [r[\"sell_ratio\"] for r in valid_months]\n",
        "            avg_ratio = float(np.mean(ratios)) if ratios else 0.0\n",
        "            std_ratio = float(np.std(ratios)) if ratios else 0.0\n",
        "            \n",
        "            print(f\"   ðŸ“Š Overall SELL: {overall_ratio:.2f}% ({total_sell:,}/{total_samples:,})\")\n",
        "            print(f\"   ðŸ“… Monthly consistency: {consistency:.1f}% ({months_with_sell}/{len(valid_months)} months)\")\n",
        "            print(f\"   Avg monthly SELL: {avg_ratio:.2f}% | Std: {std_ratio:.2f}%\")\n",
        "            \n",
        "            results.append({\n",
        "                \"window\": config['window'],\n",
        "                \"upper\": config['upper'],\n",
        "                \"lower\": config['lower'],\n",
        "                \"name\": config['name'],\n",
        "                \"overall_ratio\": overall_ratio,\n",
        "                \"consistency\": consistency,\n",
        "                \"avg_monthly\": avg_ratio,\n",
        "                \"std_monthly\": std_ratio,\n",
        "            })\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   âŒ Error: {e}\")\n",
        "            continue\n",
        "    \n",
        "    # Summary table\n",
        "    print(\"\\nðŸ“Š Summary (Specific Combinations)\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"{'Config':<15} {'Overall%':<10} {'MonthlyAvg%':<12} {'Consistency%':<12} {'Std%':<8}\")\n",
        "    print(\"-\" * 80)\n",
        "    for r in results:\n",
        "        print(f\"{r['name']:<15} {r['overall_ratio']:>8.2f}% {r['avg_monthly']:>10.2f}% {r['consistency']:>10.1f}% {r['std_monthly']:>6.2f}%\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"ðŸš€ Testing specific combinations...\")\n",
        "specific_results = test_specific_combinations(h4_full)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>open</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>close</th>\n",
              "      <th>volume</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>timestamp</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2025-10-27 08:00:00</th>\n",
              "      <td>115554.59</td>\n",
              "      <td>115607.48</td>\n",
              "      <td>114814.69</td>\n",
              "      <td>115362.02</td>\n",
              "      <td>3701.92326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-10-27 12:00:00</th>\n",
              "      <td>115362.02</td>\n",
              "      <td>115437.48</td>\n",
              "      <td>114503.99</td>\n",
              "      <td>114969.68</td>\n",
              "      <td>4493.43933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-10-27 16:00:00</th>\n",
              "      <td>114969.68</td>\n",
              "      <td>115790.00</td>\n",
              "      <td>114790.81</td>\n",
              "      <td>114942.64</td>\n",
              "      <td>2258.20754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-10-27 20:00:00</th>\n",
              "      <td>114942.64</td>\n",
              "      <td>114942.64</td>\n",
              "      <td>113830.01</td>\n",
              "      <td>114107.65</td>\n",
              "      <td>2236.06411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-10-28 00:00:00</th>\n",
              "      <td>114107.65</td>\n",
              "      <td>114547.20</td>\n",
              "      <td>113777.01</td>\n",
              "      <td>113777.01</td>\n",
              "      <td>1435.83458</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          open       high        low      close      volume\n",
              "timestamp                                                                  \n",
              "2025-10-27 08:00:00  115554.59  115607.48  114814.69  115362.02  3701.92326\n",
              "2025-10-27 12:00:00  115362.02  115437.48  114503.99  114969.68  4493.43933\n",
              "2025-10-27 16:00:00  114969.68  115790.00  114790.81  114942.64  2258.20754\n",
              "2025-10-27 20:00:00  114942.64  114942.64  113830.01  114107.65  2236.06411\n",
              "2025-10-28 00:00:00  114107.65  114547.20  113777.01  113777.01  1435.83458"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "h4_full.tail(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ’¾ Saving focused feature sets...\n",
            "ðŸŽ¯ Creating target variable (50-period window, first threshold logic)...\n",
            "ðŸ”„ Calculating target labels (this might take a while)...\n",
            "   Processing 0/12403 records...\n",
            "   Processing 1000/12403 records...\n",
            "   Processing 2000/12403 records...\n",
            "   Processing 3000/12403 records...\n",
            "   Processing 4000/12403 records...\n",
            "   Processing 5000/12403 records...\n",
            "   Processing 6000/12403 records...\n",
            "   Processing 7000/12403 records...\n",
            "   Processing 8000/12403 records...\n",
            "   Processing 9000/12403 records...\n",
            "   Processing 10000/12403 records...\n",
            "   Processing 11000/12403 records...\n",
            "   Processing 12000/12403 records...\n",
            "Filtering results to the specified date range...\n",
            "âœ… Target variable created:\n",
            "   Total records in focused period: 11737\n",
            "   Sell labels (target=1): 1560\n",
            "   Rest/Buy labels (target=0): 10177\n",
            "ðŸŽ‰ All feature sets saved successfully!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>timestamp</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2020-05-12 00:00:00</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-05-12 04:00:00</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-05-12 08:00:00</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-05-12 12:00:00</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-05-12 16:00:00</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-09-18 08:00:00</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-09-18 12:00:00</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-09-18 16:00:00</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-09-18 20:00:00</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-09-19 00:00:00</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11737 rows Ã— 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                     target\n",
              "timestamp                  \n",
              "2020-05-12 00:00:00       0\n",
              "2020-05-12 04:00:00       0\n",
              "2020-05-12 08:00:00       0\n",
              "2020-05-12 12:00:00       0\n",
              "2020-05-12 16:00:00       0\n",
              "...                     ...\n",
              "2025-09-18 08:00:00       0\n",
              "2025-09-18 12:00:00       0\n",
              "2025-09-18 16:00:00       0\n",
              "2025-09-18 20:00:00       0\n",
              "2025-09-19 00:00:00       0\n",
              "\n",
              "[11737 rows x 1 columns]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def create_and_save_y(h4_full, train_start='2020-05-12', test_end='2025-09-19'):\n",
        "    \"\"\"Save focused feature sets with correct target variable creation\"\"\"\n",
        "    \n",
        "    print(\"ðŸ’¾ Saving focused feature sets...\")\n",
        "    \n",
        "    # Create features directory\n",
        "    features_dir = Path('../features')\n",
        "    features_dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    # Create target variable using FULL H4 data\n",
        "    y_focused = create_target_variable_first_threshold(h4_full,\n",
        "        window_size=50,\n",
        "        upper_threshold=0.10,\n",
        "        lower_threshold=-0.12,\n",
        "        train_start=train_start, test_end=test_end)\n",
        "    \n",
        "    # Save all files\n",
        "    y_focused.to_parquet(features_dir / 'y.parquet')\n",
        "    \n",
        "    print(\"ðŸŽ‰ All feature sets saved successfully!\")\n",
        "    return y_focused\n",
        "\n",
        "# Run the complete save function\n",
        "create_and_save_y(h4_full, train_start='2020-05-12', test_end='2025-09-19')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "csml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
