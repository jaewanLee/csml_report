{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# BTC Feature Engineering - CORRECTED APPROACH\n",
        "\n",
        "## Overview\n",
        "This notebook implements the CORRECTED approach: Calculate everything together, split on save.\n",
        "\n",
        "**Key Changes:**\n",
        "1. Load full data (including buffer) for complete calculations\n",
        "2. Calculate all indicators and features on full data\n",
        "3. Create feature sets A0→A4 with proper temporal alignment\n",
        "4. Split clean data only at save step\n",
        "\n",
        "**Benefits:**\n",
        "- Complete historical context for all calculations\n",
        "- Proper temporal alignment with full data\n",
        "- No missing data for lag features\n",
        "- Clean final output without buffer data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import talib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "\n",
        "train_start='2020-05-12'\n",
        "test_end='2025-09-19'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Load Full Data (Including Buffer) for Complete Calculations\n",
        "def load_full_data():\n",
        "    \"\"\"Load full data including buffer for complete calculations\"\"\"\n",
        "    \n",
        "    # Load full data\n",
        "    h4_full = pd.read_parquet('../data_collection/data/btc_4h_20251028.parquet')\n",
        "    d1_full = pd.read_parquet('../data_collection/data/btc_1d_20251028.parquet')\n",
        "    w1_full = pd.read_parquet('../data_collection/data/btc_1w_20251028.parquet')\n",
        "    # m1_full = pd.read_parquet('../data_collection/data/btc_1M_20251022.parquet')\n",
        "    \n",
        "    # Ensure datetime index\n",
        "    for df in [h4_full, d1_full, w1_full]:\n",
        "        df.index = pd.to_datetime(df.index)\n",
        "    \n",
        "    print(f\"📊 Full data loaded (including buffer):\")\n",
        "    print(f\"  H4: {len(h4_full)} records ({h4_full.index[0]} to {h4_full.index[-1]})\")\n",
        "    print(f\"  D1: {len(d1_full)} records ({d1_full.index[0]} to {d1_full.index[-1]})\")\n",
        "    print(f\"  W1: {len(w1_full)} records ({w1_full.index[0]} to {w1_full.index[-1]})\")\n",
        "    # print(f\"  M1: {len(m1_full)} records ({m1_full.index[0]} to {m1_full.index[-1]})\")\n",
        "    \n",
        "    return h4_full, d1_full, w1_full\n",
        "\n",
        "# Load full data for complete calculations\n",
        "h4_full, d1_full, w1_full = load_full_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Technical Indicator Functions\n",
        "def extract_ohlcv_features(data):\n",
        "    \"\"\"Extract OHLCV features (5 features)\"\"\"\n",
        "    features = pd.DataFrame(index=data.index)\n",
        "    features['open'] = data['open']\n",
        "    features['high'] = data['high']\n",
        "    features['low'] = data['low']\n",
        "    features['close'] = data['close']\n",
        "    features['volume'] = data['volume']\n",
        "    features['volume_MA_20'] = talib.SMA(data['volume'], timeperiod=20)\n",
        "    return features\n",
        "\n",
        "def calculate_moving_averages(data, periods=[7, 14, 20, 60, 120]):\n",
        "    \"\"\"Calculate moving averages using CLOSE prices (5 features)\"\"\"\n",
        "    features = pd.DataFrame(index=data.index)\n",
        "    for period in periods:\n",
        "        features[f'MA_{period}'] = talib.SMA(data['close'], timeperiod=period)\n",
        "    return features\n",
        "\n",
        "def calculate_rsi(data, period=14):\n",
        "    \"\"\"Calculate RSI using CLOSE prices (1 feature)\"\"\"\n",
        "    features = pd.DataFrame(index=data.index)\n",
        "    features['RSI_14'] = talib.RSI(data['close'], timeperiod=period)\n",
        "    return features\n",
        "\n",
        "def calculate_macd(data, fast=12, slow=26, signal=9):\n",
        "    \"\"\"Calculate MACD line, signal, and histogram (3 features)\"\"\"\n",
        "    features = pd.DataFrame(index=data.index)\n",
        "    macd_line, macd_signal, macd_hist = talib.MACD(data['close'], \n",
        "                                                   fastperiod=fast, \n",
        "                                                   slowperiod=slow, \n",
        "                                                   signalperiod=signal)\n",
        "    features['MACD_line'] = macd_line\n",
        "    features['MACD_signal'] = macd_signal\n",
        "    features['MACD_hist'] = macd_hist\n",
        "    return features\n",
        "\n",
        "def calculate_ichimoku(data):\n",
        "    \"\"\"Calculate Ichimoku Cloud components (5 features)\"\"\"\n",
        "    features = pd.DataFrame(index=data.index)\n",
        "    \n",
        "    # Tenkan-sen (Conversion Line)\n",
        "    high_9 = data['high'].rolling(window=9).max()\n",
        "    low_9 = data['low'].rolling(window=9).min()\n",
        "    features['conversion_line'] = (high_9 + low_9) / 2\n",
        "    \n",
        "    # Kijun-sen (Baseline)\n",
        "    high_26 = data['high'].rolling(window=26).max()\n",
        "    low_26 = data['low'].rolling(window=26).min()\n",
        "    features['baseline'] = (high_26 + low_26) / 2\n",
        "    \n",
        "    # Senkou Span A (Leading Span A)\n",
        "    span_a_value = (features['conversion_line'] + features['baseline']) / 2\n",
        "    features['leading_span_A'] = span_a_value.shift(26)\n",
        "    # features['leading_span_A'] = (features['conversion_line'] + features['baseline']) / 2\n",
        "    \n",
        "    # Senkou Span B (Leading Span B)\n",
        "    high_52 = talib.MAX(data['high'], timeperiod=52)\n",
        "    low_52 = talib.MIN(data['low'], timeperiod=52)\n",
        "    span_b_value = (high_52 + low_52) / 2\n",
        "    features['leading_span_B'] = span_b_value.shift(26)\n",
        "\n",
        "    # high_52 = data['high'].rolling(window=52).max()\n",
        "    # low_52 = data['low'].rolling(window=52).min()\n",
        "    # features['leading_span_B'] = (high_52 + low_52) / 2\n",
        "    \n",
        "    # Chikou Span (Lagging Span) - Current close compared to 26 periods ago\n",
        "    features['lagging_span'] = data['close'].shift(26)\n",
        "    \n",
        "    return features\n",
        "\n",
        "def calculate_all_indicators(data, timeframe_name):\n",
        "    \"\"\"Calculate all 19 indicators for a timeframe\"\"\"\n",
        "    print(f\"Calculating indicators for {timeframe_name}...\")\n",
        "    \n",
        "    # Combine all indicator functions\n",
        "    ohlcv = extract_ohlcv_features(data)\n",
        "    ma = calculate_moving_averages(data)\n",
        "    rsi = calculate_rsi(data)\n",
        "    macd = calculate_macd(data)\n",
        "    ichimoku = calculate_ichimoku(data)\n",
        "    \n",
        "    # Combine all features\n",
        "    all_features = pd.concat([ohlcv, ma, rsi, macd, ichimoku], axis=1)\n",
        "    \n",
        "    # Add timeframe prefix to column names\n",
        "    # all_features.columns = [f\"{timeframe_name}_{col}\" for col in all_features.columns]\n",
        "    \n",
        "    print(f\"✅ {timeframe_name}: {len(all_features.columns)} features created\")\n",
        "    return all_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from typing import List, Optional\n",
        "from itertools import combinations\n",
        "\n",
        "def normalize_moving_averages(data:pd.DataFrame,periods:Optional=[7,14,20,60,120])->pd.DataFrame:\n",
        "    \"\"\"Normalize moving averages by dividing by close price\"\"\"\n",
        "    new_features = {}\n",
        "\n",
        "    for period in periods:\n",
        "        if f'MA_{period}' not in data.columns:\n",
        "            print(f\"Warning: {f'MA_{period}'} not found. Skipping.\")\n",
        "            continue\n",
        "        ma_col = f'MA_{period}'\n",
        "        new_feature_name = f'{ma_col}_norm'\n",
        "        normalized_val = (data['close'] / data[ma_col]) - 1\n",
        "        new_features[new_feature_name] = normalized_val.replace([np.inf, -np.inf], 0)\n",
        "    for (p_short,p_long) in combinations(periods,2):\n",
        "        if f'MA_{p_short}' not in data.columns or f'MA_{p_long}' not in data.columns:\n",
        "            print(f\"Warning: {f'MA_{p_short}'} or {f'MA_{p_long}'} not found. Skipping.\")\n",
        "            continue\n",
        "        ma_col_short = f'MA_{p_short}'\n",
        "        ma_col_long = f'MA_{p_long}'\n",
        "        new_feature_name = f'{ma_col_short}_{ma_col_long}_norm'\n",
        "        normalized_val = (data[ma_col_short] / data[ma_col_long]) - 1\n",
        "        new_features[new_feature_name] = normalized_val.replace([np.inf, -np.inf], 0)\n",
        "\n",
        "    for col_name,values in new_features.items():\n",
        "        data[col_name] = values\n",
        "\n",
        "    return data\n",
        "\n",
        "def normalize_ichimoku(data: pd.DataFrame) -> pd.DataFrame:\n",
        "    new_features = {}\n",
        "    \n",
        "    # 원본 Ichimoku 라인 이름 목록\n",
        "    ichimoku_lines = [\n",
        "        'conversion_line', \n",
        "        'baseline', \n",
        "        'leading_span_A', \n",
        "        'leading_span_B', \n",
        "        'lagging_span'\n",
        "    ]\n",
        "    \n",
        "    # --- 1. (Close vs. 원본 Ichimoku Line) 계산 ---\n",
        "    for line_col in ichimoku_lines:\n",
        "        if line_col not in data.columns:\n",
        "            print(f\"Warning: {line_col} not found. Skipping.\")\n",
        "            continue\n",
        "            \n",
        "        new_feature_name = f'close_vs_{line_col}_pct'\n",
        "        # (close / line) - 1\n",
        "        normalized_val = (data['close'] / data[line_col]) - 1\n",
        "        new_features[new_feature_name] = normalized_val.replace([np.inf, -np.inf, np.nan], 0) # NaN도 0으로 처리\n",
        "\n",
        "    # --- 2. (원본 Line vs. 원본 Line) 계산 (크로스) ---\n",
        "    \n",
        "    # 전환선 vs 기준선\n",
        "    if 'conversion_line' in data.columns and 'baseline' in data.columns:\n",
        "        new_feature_name = 'conversion_vs_baseline_pct'\n",
        "        # (conversion / baseline) - 1\n",
        "        normalized_val = (data['conversion_line'] / data['baseline']) - 1\n",
        "        new_features[new_feature_name] = normalized_val.replace([np.inf, -np.inf, np.nan], 0)\n",
        "\n",
        "    # 선행스팬 A vs 선행스팬 B (구름 관계)\n",
        "    if 'leading_span_A' in data.columns and 'leading_span_B' in data.columns:\n",
        "        new_feature_name = 'span_A_vs_span_B_pct'\n",
        "        # (Span A / Span B) - 1\n",
        "        normalized_val = (data['leading_span_A'] / data['leading_span_B']) - 1\n",
        "        new_features[new_feature_name] = normalized_val.replace([np.inf, -np.inf, np.nan], 0)\n",
        "\n",
        "    # --- 3. 계산된 모든 피처를 원본 DataFrame에 한 번에 추가 ---\n",
        "    for col_name, values in new_features.items():\n",
        "        data[col_name] = values\n",
        "            \n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "def normalize_candle_features(data: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    OHLC 값을 기반으로 정규화된 캔들 모양 피처를 계산하여 추가합니다.\n",
        "\n",
        "    Args:\n",
        "        data: 'open', 'high', 'low', 'close' 컬럼이 포함된 DataFrame\n",
        "\n",
        "    Returns:\n",
        "        정규화된 캔들 피처('candle_body_pct', 'high_wick_pct',\n",
        "        'low_wick_pct', 'range_pct')가 추가된 DataFrame\n",
        "    \"\"\"\n",
        "    # 계산된 새 피처들을 임시 저장\n",
        "    new_features = {}\n",
        "\n",
        "    # 1. 캔들 몸통 비율 (종가 변화율)\n",
        "    # (close - open) / open\n",
        "    open_price = data['open']\n",
        "    close_price = data['close']\n",
        "    new_features['candle_body_pct'] = np.where(\n",
        "        open_price != 0,\n",
        "        (close_price - open_price) / open_price,\n",
        "        0\n",
        "    )\n",
        "\n",
        "    # 2. 윗꼬리 비율\n",
        "    # (high - max(open, close)) / close\n",
        "    high_price = data['high']\n",
        "    body_top = np.maximum(open_price, close_price) # 몸통 상단 (양봉이면 close, 음봉이면 open)\n",
        "    new_features['high_wick_pct'] = np.where(\n",
        "        close_price != 0,\n",
        "        (high_price - body_top) / close_price,\n",
        "        0\n",
        "    )\n",
        "\n",
        "    # 3. 아랫꼬리 비율\n",
        "    # (min(open, close) - low) / close\n",
        "    low_price = data['low']\n",
        "    body_bottom = np.minimum(open_price, close_price) # 몸통 하단 (양봉이면 open, 음봉이면 close)\n",
        "    new_features['low_wick_pct'] = np.where(\n",
        "        close_price != 0,\n",
        "        (body_bottom - low_price) / close_price,\n",
        "        0\n",
        "    )\n",
        "\n",
        "    # 4. 캔들 전체 범위 비율\n",
        "    # (high - low) / low\n",
        "    new_features['range_pct'] = np.where(\n",
        "        low_price != 0,\n",
        "        (high_price - low_price) / low_price,\n",
        "        0\n",
        "    )\n",
        "\n",
        "    # 계산된 모든 피처를 원본 DataFrame에 한 번에 추가\n",
        "    for col_name, values in new_features.items():\n",
        "        data[col_name] = values\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def normalize_volume_features(data: pd.DataFrame, ma_periods: Optional[List[int]] = [20]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Volume 피처를 두 가지 방식으로 정규화하여 새 컬럼으로 추가합니다.\n",
        "    1. (현재 Volume vs. Volume MA) 이격도\n",
        "    2. (이전 캔들 대비 Volume 변화율)\n",
        "\n",
        "    Args:\n",
        "        data: 'volume' 및 계산된 Volume MA 컬럼('volume_MA_20' 등)이 포함된 DataFrame\n",
        "        ma_periods: Volume MA 계산에 사용된 기간 목록\n",
        "\n",
        "    Returns:\n",
        "        정규화된 Volume 피처('volume_vs_MA_20_pct', 'volume_change_pct')가 추가된 DataFrame\n",
        "    \"\"\"\n",
        "\n",
        "    # 계산된 새 피처들을 임시 저장\n",
        "    new_features = {}\n",
        "\n",
        "    # --- 1. (Volume vs. Volume MA) 이격도 계산 ---\n",
        "    if 'volume' in data.columns:\n",
        "        for period in ma_periods:\n",
        "            ma_col = f'volume_MA_{period}'\n",
        "            new_feature_name = f'volume_vs_{ma_col}_pct'\n",
        "            \n",
        "            if ma_col in data.columns:\n",
        "                # (volume / volume_MA) - 1\n",
        "                normalized_val = (data['volume'] / data[ma_col]) - 1\n",
        "                new_features[new_feature_name] = normalized_val.replace([np.inf, -np.inf, np.nan], 0)\n",
        "            else:\n",
        "                 print(f\"Warning: {ma_col} not found for normalization. Skipping {new_feature_name}.\")\n",
        "\n",
        "\n",
        "    # --- 2. (이전 캔들 대비 Volume 변화율) 계산 ---\n",
        "    if 'volume' in data.columns:\n",
        "        new_feature_name = 'volume_change_pct'\n",
        "        # .pct_change()는 이전 값 대비 변화율을 계산합니다.\n",
        "        # 첫 번째 값은 NaN이 되므로 fillna(0)으로 처리합니다.\n",
        "        new_features[new_feature_name] = data['volume'].pct_change().fillna(0).replace([np.inf, -np.inf], 0)\n",
        "    else:\n",
        "        print(\"Warning: 'volume' column not found. Skipping volume_change_pct.\")\n",
        "\n",
        "\n",
        "    # --- 3. 계산된 모든 피처를 원본 DataFrame에 한 번에 추가 ---\n",
        "    for col_name, values in new_features.items():\n",
        "        data[col_name] = values\n",
        "\n",
        "    return data\n",
        "\n",
        "def normalize_all_features(data: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Normalize all features\"\"\"\n",
        "    data = normalize_moving_averages(data)\n",
        "    data = normalize_ichimoku(data)\n",
        "    data = normalize_candle_features(data)\n",
        "    data = normalize_volume_features(data)\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "h4_full.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from typing import List, Optional\n",
        "\n",
        "def remove_absolute_value_features(data: pd.DataFrame, \n",
        "                                   ma_periods: Optional[List[int]] = [7, 14, 20, 60, 120],\n",
        "                                   volume_ma_periods: Optional[List[int]] = [20]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Removes the original absolute value columns (OHLC, Volume, MA, Ichimoku) \n",
        "    from the DataFrame, keeping only the normalized features.\n",
        "\n",
        "    Args:\n",
        "        data: DataFrame containing both original and normalized features.\n",
        "        ma_periods: List of periods used for price moving averages.\n",
        "        volume_ma_periods: List of periods used for volume moving averages.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with absolute value columns removed.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Columns to potentially remove\n",
        "    cols_to_remove = []\n",
        "\n",
        "    # 1. Original OHLC\n",
        "    cols_to_remove.extend(['open', 'high', 'low', 'close'])\n",
        "\n",
        "    # 2. Original Volume\n",
        "    cols_to_remove.append('volume')\n",
        "\n",
        "    # 3. Original Price Moving Averages\n",
        "    for period in ma_periods:\n",
        "        cols_to_remove.append(f'MA_{period}')\n",
        "\n",
        "    # 4. Original Ichimoku Lines\n",
        "    cols_to_remove.extend([\n",
        "        'conversion_line', \n",
        "        'baseline', \n",
        "        'leading_span_A', \n",
        "        'leading_span_B', \n",
        "        'lagging_span'\n",
        "    ])\n",
        "    \n",
        "    # 5. Original Volume Moving Averages\n",
        "    for period in volume_ma_periods:\n",
        "        cols_to_remove.append(f'volume_MA_{period}')\n",
        "\n",
        "    # Check which columns actually exist in the DataFrame\n",
        "    existing_cols_to_remove = [col for col in cols_to_remove if col in data.columns]\n",
        "    \n",
        "    # Drop the existing columns\n",
        "    print(f\"Removing {len(existing_cols_to_remove)} absolute value columns: {existing_cols_to_remove}\")\n",
        "    data = data.drop(columns=existing_cols_to_remove)\n",
        "    \n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Calculate Indicators on Full Data (Including Buffer)\n",
        "def calculate_indicators_on_full_data(h4_full, d1_full, w1_full):\n",
        "    \"\"\"Calculate indicators using full data to ensure proper calculations\"\"\"\n",
        "    \n",
        "    print(\"🔄 Calculating indicators on full data (including buffer)...\")\n",
        "    \n",
        "    # Calculate indicators on full data\n",
        "    h4_indicators_full = calculate_all_indicators(h4_full, 'H4')\n",
        "    d1_indicators_full = calculate_all_indicators(d1_full, 'D1')\n",
        "    w1_indicators_full = calculate_all_indicators(w1_full, 'W1')\n",
        "    print(h4_indicators_full.columns)\n",
        "    print(d1_indicators_full.columns)\n",
        "    print(w1_indicators_full.columns)\n",
        "\n",
        "    # normalize\n",
        "    h4_indicators_full = normalize_all_features(h4_indicators_full)\n",
        "    d1_indicators_full = normalize_all_features(d1_indicators_full)\n",
        "    w1_indicators_full = normalize_all_features(w1_indicators_full)\n",
        "    \n",
        "    print(h4_indicators_full.columns)\n",
        "    print(d1_indicators_full.columns)\n",
        "    print(w1_indicators_full.columns)\n",
        "\n",
        "    # remove absolute value features\n",
        "    h4_indicators_full = remove_absolute_value_features(h4_indicators_full)\n",
        "    d1_indicators_full = remove_absolute_value_features(d1_indicators_full)\n",
        "    w1_indicators_full = remove_absolute_value_features(w1_indicators_full)\n",
        "\n",
        "    print(h4_indicators_full.columns)\n",
        "    print(d1_indicators_full.columns)\n",
        "    print(w1_indicators_full.columns)   \n",
        "    \n",
        "    print(\"✅ All indicators calculated on full data\")\n",
        "    return h4_indicators_full, d1_indicators_full, w1_indicators_full\n",
        "\n",
        "# Calculate indicators on full data\n",
        "h4_indicators_full, d1_indicators_full, w1_indicators_full = calculate_indicators_on_full_data(\n",
        "    h4_full, d1_full, w1_full\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "h4_indicators_full.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "h4_used_range=h4_indicators_full[(h4_indicators_full.index>=train_start)& (h4_indicators_full.index<=test_end)] \n",
        "print(h4_used_range.isnull().sum().sum())\n",
        "print(d1_indicators_full[(d1_indicators_full.index>=train_start)& (d1_indicators_full.index<=test_end)].isnull().sum().sum())\n",
        "print(w1_indicators_full[(w1_indicators_full.index>=train_start)& (w1_indicators_full.index<=test_end)].isnull().sum().sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import calendar\n",
        "\n",
        "\n",
        "def get_previous_month_timestamp(timestamp):\n",
        "    \"\"\"\n",
        "    Get the 10th day of the previous month\n",
        "    Simple and handles all edge cases!\n",
        "    \"\"\"\n",
        "    dt = pd.to_datetime(timestamp)\n",
        "    \n",
        "    # Get previous month\n",
        "    if dt.month == 1:\n",
        "        prev_month = dt.replace(year=dt.year-1, month=12, day=10)\n",
        "    else:\n",
        "        prev_month = dt.replace(month=dt.month-1, day=10)\n",
        "    \n",
        "    return prev_month\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Step 3.5: Remove Problematic Indicators After Calculation\n",
        "# def remove_problematic_indicators(h4_indicators_full, d1_indicators_full, w1_indicators_full, m1_indicators_full):\n",
        "#     \"\"\"\n",
        "#     Remove indicators that cannot be calculated with available data\n",
        "#     - W1: Remove 120 MA (needs 2.3 years of data)\n",
        "#     - M1: Remove 120 MA, 60 MA, leading_span_A, leading_span_B (need 5-10 years of data)\n",
        "#     \"\"\"\n",
        "#     print(\"🧹 Removing problematic indicators...\")\n",
        "    \n",
        "#     # W1: Remove 120 MA\n",
        "#     w1_indicators_clean = w1_indicators_full.copy()\n",
        "#     if 'W1_MA_120' in w1_indicators_clean.columns:\n",
        "#         w1_indicators_clean = w1_indicators_clean.drop('W1_MA_120', axis=1)\n",
        "#         print(\"✅ Removed W1_MA_120 (needs 2.3 years of data)\")\n",
        "    \n",
        "#     # M1: Remove 120 MA, 60 MA, leading_span_A, leading_span_B\n",
        "#     m1_indicators_clean = m1_indicators_full.copy()\n",
        "#     problematic_m1_cols = ['M1_MA_120', 'M1_MA_60', 'M1_leading_span_A', 'M1_leading_span_B', 'M1_MACD_line', 'M1_MACD_signal', 'M1_MACD_hist']\n",
        "    \n",
        "#     for col in problematic_m1_cols:\n",
        "#         if col in m1_indicators_clean.columns:\n",
        "#             m1_indicators_clean = m1_indicators_clean.drop(col, axis=1)\n",
        "#             print(f\"✅ Removed {col} (needs 5-10 years of data)\")\n",
        "    \n",
        "#     print(f\"📊 Cleaned indicators:\")\n",
        "#     print(f\"  H4: {len(h4_indicators_full.columns)} features (no changes)\")\n",
        "#     print(f\"  D1: {len(d1_indicators_full.columns)} features (no changes)\")\n",
        "#     print(f\"  W1: {len(w1_indicators_clean.columns)} features (removed 1)\")\n",
        "#     print(f\"  M1: {len(m1_indicators_clean.columns)} features (removed 7)\")\n",
        "    \n",
        "#     return h4_indicators_full, d1_indicators_full, w1_indicators_clean, m1_indicators_clean\n",
        "\n",
        "# # Remove problematic indicators\n",
        "# h4_indicators_clean, d1_indicators_clean, w1_indicators_clean = remove_problematic_indicators(\n",
        "#     h4_indicators_full, d1_indicators_full, w1_indicators_full\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Temporal Alignment Functions (CORRECTED VERSION)\n",
        "def align_timeframe_data(base_data, target_data, base_timeframe, target_timeframe):\n",
        "    \"\"\"\n",
        "    Align target timeframe data with base timeframe data using proper temporal alignment\n",
        "    \n",
        "    Args:\n",
        "        base_data: H4 data (base timeframe)\n",
        "        target_data: D1/W1/M1 data (target timeframe)\n",
        "        base_timeframe: 'H4'\n",
        "        target_timeframe: 'D1', 'W1', 'M1', 'D1_lags', 'W1_lags', 'M1_lags'\n",
        "    \n",
        "    Returns:\n",
        "        aligned_data: Target data aligned with base data timestamps\n",
        "    \"\"\"\n",
        "    print(f\"🔄 Aligning {target_timeframe} data with {base_timeframe} timestamps...\")\n",
        "    \n",
        "    # Define timeframe offsets - ADD LAG SUPPORT\n",
        "    timeframe_offsets = {\n",
        "        'D1': pd.Timedelta(days=1),\n",
        "        'W1': pd.Timedelta(weeks=1),\n",
        "        # Add lag support\n",
        "        'D1_lags': pd.Timedelta(days=1),\n",
        "        'W1_lags': pd.Timedelta(weeks=1)\n",
        "    }\n",
        "    \n",
        "    aligned_data = pd.DataFrame(index=base_data.index, columns=target_data.columns)\n",
        "    \n",
        "    for base_timestamp in base_data.index:\n",
        "        # Use regular timedelta for other timeframes\n",
        "        offset = timeframe_offsets[target_timeframe]\n",
        "        cutoff_time = base_timestamp - offset\n",
        "        \n",
        "        # Find target data that is <= cutoff_time (previous completed data)\n",
        "        available_target_data = target_data[target_data.index <= cutoff_time]\n",
        "        \n",
        "        if len(available_target_data) > 0:\n",
        "            # Use the most recent available data (previous completed)\n",
        "            latest_target_data = available_target_data.iloc[-1]\n",
        "            aligned_data.loc[base_timestamp] = latest_target_data\n",
        "        else:\n",
        "            # If no data available, fill with NaN\n",
        "            aligned_data.loc[base_timestamp] = np.nan\n",
        "    \n",
        "    print(f\"✅ {target_timeframe} data aligned: {len(aligned_data.columns)} features, {len(aligned_data)} records\")\n",
        "    return aligned_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Create Feature Sets A0→A3 with Cleaned Indicators\n",
        "def create_feature_sets_with_cleaned_indicators(h4, d1, w1):\n",
        "    \"\"\"Create feature sets A0→A3 using cleaned indicators (no problematic indicators)\"\"\"\n",
        "\n",
        "    h4.columns=[f\"H4_{col}\" for col in h4.columns]\n",
        "    d1.columns=[f\"D1_{col}\" for col in d1.columns]\n",
        "    w1.columns=[f\"W1_{col}\" for col in w1.columns]\n",
        "    \n",
        "    # A0: H4 indicators only\n",
        "    A0 = h4.copy()\n",
        "    \n",
        "    # A1: H4 + D1 indicators - Align D1 with H4 timestamps\n",
        "    d1_aligned = align_timeframe_data(A0, d1, 'H4', 'D1')\n",
        "    A1 = pd.concat([h4, d1_aligned], axis=1)\n",
        "    \n",
        "    # A2: H4 + D1 + W1 indicators - Align W1 with H4 timestamps\n",
        "    w1_aligned = align_timeframe_data(A0, w1, 'H4', 'W1')\n",
        "    A2 = pd.concat([h4, d1_aligned, w1_aligned], axis=1)\n",
        "    \n",
        "    \n",
        "    print(f\"✅ Feature sets A0→A3 created with cleaned indicators:\")\n",
        "    print(f\"  A0: {len(A0.columns)} features, {len(A0)} records\")\n",
        "    print(f\"  A1: {len(A1.columns)} features, {len(A1)} records\")\n",
        "    print(f\"  A2: {len(A2.columns)} features, {len(A2)} records\")\n",
        "    \n",
        "    return A0, A1, A2\n",
        "\n",
        "# Create feature sets with cleaned indicators\n",
        "A0, A1, A2 = create_feature_sets_with_cleaned_indicators(\n",
        "    h4_indicators_full, d1_indicators_full, w1_indicators_full\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate_A0_to_A3(A0, A1, A2):\n",
        "    \"\"\"Validate feature sets A0→A3\"\"\"\n",
        "    print(\"🔍 Feature Set Validation:\")\n",
        "    train_start = '2020-05-12'\n",
        "    test_end = '2025-09-19'\n",
        "\n",
        "    A0_focused = A0[(A0.index >= train_start) & (A0.index <= test_end)]\n",
        "    A1_focused = A1[(A1.index >= train_start) & (A1.index <= test_end)]\n",
        "    A2_focused = A2[(A2.index >= train_start) & (A2.index <= test_end)]\n",
        "\n",
        "    print(\n",
        "        f\"A0_focused.isnull().sum().sum(): {A0_focused.isnull().sum().sum()}\")\n",
        "    print(\n",
        "        f\"A1_focused.isnull().sum().sum(): {A1_focused.isnull().sum().sum()}\")\n",
        "    print(\n",
        "        f\"A2_focused.isnull().sum().sum(): {A2_focused.isnull().sum().sum()}\")\n",
        "\n",
        "validate_A0_to_A3(A0, A1, A2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 6: Create Historical Lag Features (Using Full Data with Buffer)\n",
        "def create_lag_features(indicators_full, timeframe_name, lag_periods):\n",
        "    \"\"\"Create historical lag features for a timeframe using full data (including buffer)\"\"\"\n",
        "    lag_features = pd.DataFrame(index=indicators_full.index)\n",
        "    \n",
        "    for lag in lag_periods:\n",
        "        for col in indicators_full.columns:\n",
        "            lag_features[f\"{col}_lag_{lag}\"] = indicators_full[col].shift(lag)\n",
        "    \n",
        "    print(f\"✅ {timeframe_name} lags: {len(lag_features.columns)} features created\")\n",
        "    return lag_features\n",
        "\n",
        "def create_all_lag_features_with_buffer(h4_indicators_clean, d1_indicators_clean, w1_indicators_clean):\n",
        "    \"\"\"Create historical lag features for all timeframes using full data (including buffer)\"\"\"\n",
        "    \n",
        "    print(\"⏰ Creating historical lag features using full data (including buffer)...\")\n",
        "    \n",
        "    # H4 lags: t-1 to t-6 (6 lags)\n",
        "    h4_lags_full = create_lag_features(h4_indicators_clean, 'H4', range(1, 7))\n",
        "    \n",
        "    # D1 lags: t-1 to t-7 (7 lags)\n",
        "    d1_lags_full = create_lag_features(d1_indicators_clean, 'D1', range(1, 8))\n",
        "    \n",
        "    # W1 lags: t-1 to t-4 (4 lags)\n",
        "    w1_lags_full = create_lag_features(w1_indicators_clean, 'W1', range(1, 5))\n",
        "    \n",
        "    \n",
        "    print(f\"✅ All lag features created using full data:\")\n",
        "    print(f\"  H4 lags: {len(h4_lags_full.columns)} features, {len(h4_lags_full)} records\")\n",
        "    print(f\"  D1 lags: {len(d1_lags_full.columns)} features, {len(d1_lags_full)} records\")\n",
        "    print(f\"  W1 lags: {len(w1_lags_full.columns)} features, {len(w1_lags_full)} records\")\n",
        "    \n",
        "    return h4_lags_full, d1_lags_full, w1_lags_full\n",
        "\n",
        "# Create historical lag features using full data (including buffer)\n",
        "h4_lags_full, d1_lags_full, w1_lags_full = create_all_lag_features_with_buffer(\n",
        "    A0,A1,A2\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "h4_lags_focused=h4_lags_full[(h4_lags_full.index >= train_start) & (h4_lags_full.index <= test_end)]\n",
        "print(h4_lags_focused.isnull().sum().sum())\n",
        "print(h4_lags_focused.index.min(), h4_lags_focused.index.max())\n",
        "\n",
        "d1_lags_focused = d1_lags_full[(d1_lags_full.index >= train_start)\n",
        "                               & (d1_lags_full.index <= test_end)]\n",
        "print(d1_lags_focused.isnull().sum().sum())\n",
        "print(d1_lags_focused.index.min(), d1_lags_focused.index.max())\n",
        "\n",
        "w1_lags_focused = w1_lags_full[(w1_lags_full.index >= train_start)\n",
        "                              & (w1_lags_full.index <= test_end)]\n",
        "print(w1_lags_focused.isnull().sum().sum())\n",
        "print(w1_lags_focused.index.min(), w1_lags_focused.index.max())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 7: Create A4 Feature Set with Temporal Alignment (Using Full Data)\n",
        "def create_a4_features_with_temporal_alignment(A2:pd.DataFrame, h4_lags_full:pd.DataFrame, d1_lags_full:pd.DataFrame, w1_lags_full:pd.DataFrame):\n",
        "    \"\"\"Create A4 feature set: A3 + all historical lags with proper temporal alignment\"\"\"\n",
        "    \n",
        "    # Align D1, W1, M1 lag features with H4 timestamps\n",
        "    d1_lags_aligned = align_timeframe_data(A2, d1_lags_full, 'H4', 'D1_lags')\n",
        "    w1_lags_aligned = align_timeframe_data(A2, w1_lags_full, 'H4', 'W1_lags')\n",
        "    \n",
        "    # Combine A3 with all lag features\n",
        "    h4_lags_full_aligned = h4_lags_full.copy()\n",
        "    h4_lags_full_aligned.columns = [f\"H4_lags_{col}\" for col in h4_lags_full.columns]\n",
        "    d1_lags_aligned.columns = [f\"D1_lags_{col}\" for col in d1_lags_aligned.columns]\n",
        "    w1_lags_aligned.columns = [f\"W1_lags_{col}\" for col in w1_lags_aligned.columns]\n",
        "    A3 = pd.concat([A2, h4_lags_full_aligned, d1_lags_aligned, w1_lags_aligned], axis=1)\n",
        "    \n",
        "    print(f\"✅ A4 feature set created with temporal alignment:\")\n",
        "    print(f\"  A4: {len(A3.columns)} features, {len(A3)} records\")\n",
        "    print(f\"  - Current indicators: {len(A2.columns)}\")\n",
        "    print(f\"  - Historical lags: {len(A3.columns) - len(A2.columns)}\")\n",
        "    \n",
        "    return A3\n",
        "\n",
        "# Create A4 feature set with temporal alignment\n",
        "A3 = create_a4_features_with_temporal_alignment(A2, h4_lags_full, d1_lags_full, w1_lags_full)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
        "    print(A3.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 8: Data Validation & Quality Checks\n",
        "def validate_feature_sets(A0,\n",
        "                          A1,\n",
        "                          A2,\n",
        "                          A3,\n",
        "                          train_start='2020-05-12',\n",
        "                          test_end='2025-09-19'):\n",
        "    \"\"\"Validate all feature sets\"\"\"\n",
        "    print(\"🔍 Feature Set Validation (Train/Test Period Only):\")\n",
        "    print(f\"📅 Period: {train_start} to {test_end}\")\n",
        "\n",
        "    A0_focused = A0[(A0.index >= train_start) & (A0.index <= test_end)]\n",
        "    A1_focused = A1[(A1.index >= train_start) & (A1.index <= test_end)]\n",
        "    A2_focused = A2[(A2.index >= train_start) & (A2.index <= test_end)]\n",
        "    A3_focused = A3[(A3.index >= train_start) & (A3.index <= test_end)]\n",
        "\n",
        "    feature_counts = {\n",
        "        'A0': len(A0_focused.columns),\n",
        "        'A1': len(A1_focused.columns),\n",
        "        'A2': len(A2_focused.columns),\n",
        "        'A3': len(A3_focused.columns)\n",
        "    }\n",
        "\n",
        "    # A0 : 19, A1 : 38, A2 : 38 + 19 - 1= 56,  A3 : 56 + 19 - 4 = 71, A4 : \n",
        "    expected_counts = {'A0': 19, 'A1': 38, 'A2': 56, 'A3': 68}\n",
        "\n",
        "    print(\"🔍 Feature Set Validation:\")\n",
        "    for set_name, count in feature_counts.items():\n",
        "        expected = expected_counts[set_name]\n",
        "        status = \"✅\" if count == expected else \"❌\"\n",
        "        print(f\"  {status} {set_name}: {count}/{expected} features\")\n",
        "\n",
        "    # Check for missing values in focused period only\n",
        "    print(\"\\n🔍 Missing Values Check (Train/Test Period Only):\")\n",
        "    for set_name, features in [('A0', A0_focused), ('A1', A1_focused),\n",
        "                               ('A2', A2_focused), ('A3', A3_focused)]:\n",
        "        missing_count = features.isnull().sum().sum()\n",
        "        total_cells = features.shape[0] * features.shape[1]\n",
        "        missing_percentage = (missing_count / total_cells) * 100\n",
        "        print(\n",
        "            f\"  {set_name}: {missing_count:,} missing values ({missing_percentage:.2f}%)\"\n",
        "        )\n",
        "        print(f\"    Period: {features.index[0]} to {features.index[-1]}\")\n",
        "        print(f\"    Records: {len(features)}\")\n",
        "\n",
        "    return feature_counts, (A0_focused, A1_focused, A2_focused, A3_focused)\n",
        "\n",
        "# Validate feature sets\n",
        "validation_results_focused, focused_sets = validate_feature_sets(A0, A1, A2, A3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "*_,A3_focused=focused_sets\n",
        "\n",
        "# Check if M1 alignment is working correctly\n",
        "print(\"M1 temporal alignment check:\")\n",
        "print(f\"A3 missing values: {A3_focused.isnull().sum().sum()}\")\n",
        "print(\n",
        "    f\"A3 M1 columns missing: {A3_focused.filter(regex='M1_').isnull().sum().sum()}\"\n",
        ")\n",
        "\n",
        "# Check lag feature missing values\n",
        "print(\"Lag feature missing values:\")\n",
        "print(f\"A4 H4 lags missing: {A3_focused.filter(regex='H4_.*_lag_').isnull().sum().sum()}\")\n",
        "print(f\"A4 D1 lags missing: {A3_focused.filter(regex='D1_.*_lag_').isnull().sum().sum()}\")\n",
        "print(f\"A4 W1 lags missing: {A3_focused.filter(regex='W1_.*_lag_').isnull().sum().sum()}\")\n",
        "print(f\"A4 M1 lags missing: {A3_focused.filter(regex='M1_.*_lag_').isnull().sum().sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "duplicated_cols = A3.columns[A3.columns.duplicated()]\n",
        "print(f\"\\n중복된 컬럼명: {duplicated_cols.tolist()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def create_target_variable_first_threshold(h4_full: pd.DataFrame,\n",
        "                                                window_size: int = 30, # <-- 30봉으로 변경\n",
        "                                                upper_threshold: float = 0.10, # +10%\n",
        "                                                lower_threshold: float = -0.15, # -15%\n",
        "                                                train_start: str = '2020-05-12',\n",
        "                                                test_end: str = '2025-09-19') -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Create target variable using first threshold logic (slow loop version).\n",
        "    y=0: upper_threshold hit first OR neither hit.\n",
        "    y=1: lower_threshold hit first.\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"🎯 Creating target variable ({window_size}-period window, first threshold logic)...\")\n",
        "\n",
        "    # Create target variable for ALL H4 data\n",
        "    y_full = pd.DataFrame(index=h4_full.index)\n",
        "    y_full['target'] = 0  # Initialize with 0 (REST/BUY state)\n",
        "\n",
        "    print(\"🔄 Calculating target labels (this might take a while)...\")\n",
        "\n",
        "    total_len = len(h4_full)\n",
        "    for i in range(total_len):\n",
        "        # Print progress\n",
        "        if i % 1000 == 0:\n",
        "            print(f\"   Processing {i}/{total_len} records...\")\n",
        "\n",
        "        # Ensure there's enough future data for the window\n",
        "        if i + window_size >= total_len:\n",
        "            # Not enough future data, keep default y=0 and continue to next i\n",
        "            continue\n",
        "\n",
        "        current_close = h4_full.iloc[i]['close']\n",
        "        \n",
        "        # Avoid division by zero if current_close is 0\n",
        "        if current_close == 0:\n",
        "            continue # Keep default y=0\n",
        "\n",
        "        # Get the next `window_size` periods\n",
        "        future_data = h4_full.iloc[i + 1 : i + 1 + window_size] # <-- 180 대신 window_size 사용\n",
        "\n",
        "        # Calculate price changes relative to current close\n",
        "        price_increases = (future_data['close'] - current_close) / current_close\n",
        "        price_drops = (future_data['low'] - current_close) / current_close # Check drops against future 'low'\n",
        "\n",
        "        # Find the first threshold hit within the window\n",
        "        target_set = False # Flag to check if target was set inside the inner loop\n",
        "        for j in range(len(future_data)):\n",
        "            # Check lower threshold (-15%)\n",
        "            if price_drops.iloc[j] <= lower_threshold:\n",
        "                y_full.iloc[i, 0] = 1  # Lower hit first -> SELL\n",
        "                target_set = True\n",
        "                break # Exit inner loop once a threshold is hit\n",
        "            \n",
        "            # Check upper threshold (+10%)\n",
        "            if price_increases.iloc[j] >= upper_threshold:\n",
        "                y_full.iloc[i, 0] = 0  # Upper hit first -> REST/BUY\n",
        "                target_set = True\n",
        "                break # Exit inner loop once a threshold is hit\n",
        "\n",
        "        # If the inner loop finished without hitting either threshold (no break)\n",
        "        # the default y=0 (initialized at the beginning) remains.\n",
        "\n",
        "    # --- 필터링 위치 수정 ---\n",
        "    # The main loop is finished, now filter the results\n",
        "    print(\"Filtering results to the specified date range...\")\n",
        "    y_focused = y_full[(y_full.index >= train_start)\n",
        "                       & (y_full.index <= test_end)].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
        "\n",
        "    print(f\"✅ Target variable created:\")\n",
        "    print(f\"   Total records in focused period: {len(y_focused)}\")\n",
        "    print(f\"   Sell labels (target=1): {y_focused['target'].sum()}\")\n",
        "    print(f\"   Rest/Buy labels (target=0): {len(y_focused) - y_focused['target'].sum()}\")\n",
        "\n",
        "    return y_focused\n",
        "\n",
        "# --- 함수 호출 예시 ---\n",
        "# y_target = create_target_variable_first_threshold_slow(h4_full)\n",
        "# print(y_target.head())\n",
        "# print(y_target['target'].value_counts())\n",
        "\n",
        "# create_target_variable_first_threshold(h4_full)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_focused_feature_sets_complete(A0, A1, A2, A3, h4_full, train_start='2020-05-12', test_end='2025-09-19'):\n",
        "    \"\"\"Save focused feature sets with correct target variable creation\"\"\"\n",
        "    \n",
        "    print(\"💾 Saving focused feature sets...\")\n",
        "    \n",
        "    # Create features directory\n",
        "    features_dir = Path('../features')\n",
        "    features_dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    # Filter feature sets to focused period\n",
        "    def filter_focused_period(data, start_date, end_date):\n",
        "        return data[(data.index >= start_date) & (data.index <= end_date)]\n",
        "    \n",
        "    # Save feature sets\n",
        "    A0_focused = filter_focused_period(A0, train_start, test_end)\n",
        "    A1_focused = filter_focused_period(A1, train_start, test_end)\n",
        "    A2_focused = filter_focused_period(A2, train_start, test_end)\n",
        "    A3_focused = filter_focused_period(A3, train_start, test_end)\n",
        "    \n",
        "    # Create target variable using FULL H4 data\n",
        "    y_focused = create_target_variable_first_threshold(h4_full,train_start=train_start, test_end=test_end)\n",
        "    \n",
        "    # Save all files\n",
        "    A0_focused.to_parquet(features_dir / 'A0.parquet')\n",
        "    A1_focused.to_parquet(features_dir / 'A1.parquet')\n",
        "    A2_focused.to_parquet(features_dir / 'A2.parquet')\n",
        "    A3_focused.to_parquet(features_dir / 'A3.parquet')\n",
        "    y_focused.to_parquet(features_dir / 'y.parquet')\n",
        "    \n",
        "    print(\"🎉 All feature sets saved successfully!\")\n",
        "    return A0_focused, A1_focused, A2_focused, A3_focused, y_focused\n",
        "\n",
        "# Run the complete save function\n",
        "save_focused_feature_sets_complete(A0, A1, A2, A3,  h4_full, train_start='2020-05-12', test_end='2025-09-19')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def validate_saved_feature_sets(features_dir='../features'):\n",
        "    \"\"\"\n",
        "    Validate that all feature sets and target variable were saved correctly\n",
        "    \n",
        "    Args:\n",
        "        features_dir: Path to features directory\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"🔍 Validating Saved Feature Sets...\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Check if features directory exists\n",
        "    features_path = Path(features_dir)\n",
        "    if not features_path.exists():\n",
        "        print(\"❌ Features directory not found!\")\n",
        "        return\n",
        "    \n",
        "    # Expected files\n",
        "    expected_files = ['A0.parquet', 'A1.parquet', 'A2.parquet', 'A3.parquet', 'A4.parquet', 'y.parquet']\n",
        "    \n",
        "    print(\"📁 File Existence Check:\")\n",
        "    for file in expected_files:\n",
        "        file_path = features_path / file\n",
        "        if file_path.exists():\n",
        "            print(f\"  ✅ {file} - Found\")\n",
        "        else:\n",
        "            print(f\"  ❌ {file} - Missing!\")\n",
        "    \n",
        "    print(\"\\n📊 Data Validation:\")\n",
        "    \n",
        "    # Load and validate each file\n",
        "    for file in expected_files:\n",
        "        file_path = features_path / file\n",
        "        if not file_path.exists():\n",
        "            continue\n",
        "            \n",
        "        print(f\"\\n🔍 Validating {file}:\")\n",
        "        \n",
        "        try:\n",
        "            # Load data\n",
        "            data = pd.read_parquet(file_path)\n",
        "            \n",
        "            # Basic info\n",
        "            print(f\"  📏 Shape: {data.shape[0]} records × {data.shape[1]} features\")\n",
        "            print(f\"  📅 Period: {data.index[0]} to {data.index[-1]}\")\n",
        "            \n",
        "            # Check for missing values\n",
        "            missing_count = data.isnull().sum().sum()\n",
        "            total_cells = data.shape[0] * data.shape[1]\n",
        "            missing_percentage = (missing_count / total_cells) * 100 if total_cells > 0 else 0\n",
        "            \n",
        "            if missing_count == 0:\n",
        "                print(f\"  ✅ Missing values: {missing_count} (0.00%)\")\n",
        "            else:\n",
        "                print(f\"  ⚠️ Missing values: {missing_count} ({missing_percentage:.2f}%)\")\n",
        "                \n",
        "                # Show which columns have missing values\n",
        "                missing_cols = data.isnull().sum()\n",
        "                missing_cols = missing_cols[missing_cols > 0]\n",
        "                if len(missing_cols) > 0:\n",
        "                    print(f\"    Columns with missing values:\")\n",
        "                    for col, count in missing_cols.items():\n",
        "                        print(f\"      {col}: {count} missing\")\n",
        "            \n",
        "            # Check data types\n",
        "            print(f\"  📋 Data types: {data.dtypes.value_counts().to_dict()}\")\n",
        "            \n",
        "            # Check for infinite values\n",
        "            inf_count = np.isinf(data.select_dtypes(include=[np.number])).sum().sum()\n",
        "            if inf_count == 0:\n",
        "                print(f\"  ✅ Infinite values: {inf_count}\")\n",
        "            else:\n",
        "                print(f\"  ⚠️ Infinite values: {inf_count}\")\n",
        "            \n",
        "            # Specific validation for target variable\n",
        "            if file == 'y.parquet':\n",
        "                print(f\"  🎯 Target variable validation:\")\n",
        "                print(f\"    Unique values: {data['target'].unique()}\")\n",
        "                print(f\"    Value counts: {data['target'].value_counts().to_dict()}\")\n",
        "                print(f\"    Sell percentage: {data['target'].mean()*100:.2f}%\")\n",
        "            \n",
        "            # Specific validation for feature sets\n",
        "            if file.startswith('A'):\n",
        "                print(f\"  🔢 Feature set validation:\")\n",
        "                print(f\"    Feature count: {len(data.columns)}\")\n",
        "                print(f\"    Sample features: {list(data.columns[:5])}\")\n",
        "                \n",
        "                # Check for expected feature counts\n",
        "                expected_counts = {'A0': 19, 'A1': 38, 'A2': 56, 'A3': 67, 'A4': 416}\n",
        "                if file.replace('.parquet', '') in expected_counts:\n",
        "                    expected = expected_counts[file.replace('.parquet', '')]\n",
        "                    actual = len(data.columns)\n",
        "                    if actual == expected:\n",
        "                        print(f\"    ✅ Feature count matches expected: {actual}\")\n",
        "                    else:\n",
        "                        print(f\"    ⚠️ Feature count mismatch: {actual} (expected {expected})\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ Error loading {file}: {e}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"🎉 Validation Complete!\")\n",
        "\n",
        "# Run validation\n",
        "validate_saved_feature_sets('../features')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Loading target variable for analysis...\n",
            "🎯 BTC Sell Signal Target Distribution\n",
            "============================================================\n",
            "📊 Basic Statistics:\n",
            "  Total samples: 11,737\n",
            "  SELL (target=1): 1,560 (13.29%)\n",
            "  REST (target=0): 10,177 (86.71%)\n",
            "  Class imbalance ratio: 6.52:1 (REST:SELL)\n",
            "\n",
            "📈 Value Counts:\n",
            "  REST (target=0): 10,177 (86.71%)\n",
            "  SELL (target=1): 1,560 (13.29%)\n",
            "\n",
            "📅 Temporal Distribution:\n",
            "  Yearly Distribution:\n",
            "    2020: 77 SELL / 1327 REST (5.48% SELL)\n",
            "    2021: 637 SELL / 1553 REST (29.09% SELL)\n",
            "    2022: 464 SELL / 1726 REST (21.19% SELL)\n",
            "    2023: 90 SELL / 2100 REST (4.11% SELL)\n",
            "    2024: 198 SELL / 1998 REST (9.02% SELL)\n",
            "    2025: 94 SELL / 1473 REST (6.00% SELL)\n",
            "\n",
            "  Monthly Distribution (Last 12 months):\n",
            "    2024-10: 0 SELL / 186 REST (0.00% SELL)\n",
            "    2024-11: 0 SELL / 180 REST (0.00% SELL)\n",
            "    2024-12: 12 SELL / 174 REST (6.45% SELL)\n",
            "    2025-01: 23 SELL / 163 REST (12.37% SELL)\n",
            "    2025-02: 43 SELL / 125 REST (25.60% SELL)\n",
            "    2025-03: 23 SELL / 163 REST (12.37% SELL)\n",
            "    2025-04: 5 SELL / 175 REST (2.78% SELL)\n",
            "    2025-05: 0 SELL / 186 REST (0.00% SELL)\n",
            "    2025-06: 0 SELL / 180 REST (0.00% SELL)\n",
            "    2025-07: 0 SELL / 186 REST (0.00% SELL)\n",
            "    2025-08: 0 SELL / 186 REST (0.00% SELL)\n",
            "    2025-09: 0 SELL / 109 REST (0.00% SELL)\n",
            "\n",
            "🔍 Consecutive SELL Analysis:\n",
            "  Max consecutive SELLs: 87\n",
            "  Average consecutive SELLs: 15.00\n",
            "  Total consecutive SELL sequences: 104\n",
            "\n",
            "🎯 SELL Clustering Analysis:\n",
            "  Average time between SELLs: 27.4 hours\n",
            "  Min time between SELLs: 4.0 hours\n",
            "  Max time between SELLs: 3516.0 hours\n",
            "  SELLs within 20 hours: 1491 (95.6%)\n",
            "\n",
            "📊 Target Distribution Summary:\n",
            "  This is a highly imbalanced dataset with 13.29% SELL signals\n",
            "  Class imbalance ratio: 6.5:1\n",
            "  This imbalance will require careful handling in model training\n",
            "  Consider using techniques like:\n",
            "    - Class weights (scale_pos_weight)\n",
            "    - SMOTE or other oversampling\n",
            "    - Focal Loss\n",
            "    - Stratified sampling\n"
          ]
        }
      ],
      "source": [
        "# Target Variable Distribution Analysis\n",
        "def analyze_target_distribution(y_data, title=\"Target Variable Distribution\"):\n",
        "    \"\"\"\n",
        "    Analyze and visualize target variable distribution\n",
        "    \n",
        "    Args:\n",
        "        y_data: DataFrame with 'target' column\n",
        "        title: Title for the analysis\n",
        "    \"\"\"\n",
        "    print(f\"🎯 {title}\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Basic statistics\n",
        "    total_samples = len(y_data)\n",
        "    sell_samples = y_data['target'].sum()\n",
        "    rest_samples = total_samples - sell_samples\n",
        "    sell_percentage = (sell_samples / total_samples) * 100\n",
        "    \n",
        "    print(f\"📊 Basic Statistics:\")\n",
        "    print(f\"  Total samples: {total_samples:,}\")\n",
        "    print(f\"  SELL (target=1): {sell_samples:,} ({sell_percentage:.2f}%)\")\n",
        "    print(f\"  REST (target=0): {rest_samples:,} ({100-sell_percentage:.2f}%)\")\n",
        "    \n",
        "    # Class imbalance ratio\n",
        "    imbalance_ratio = rest_samples / sell_samples\n",
        "    print(f\"  Class imbalance ratio: {imbalance_ratio:.2f}:1 (REST:SELL)\")\n",
        "    \n",
        "    # Value counts\n",
        "    print(f\"\\n📈 Value Counts:\")\n",
        "    value_counts = y_data['target'].value_counts().sort_index()\n",
        "    for value, count in value_counts.items():\n",
        "        label = \"SELL\" if value == 1 else \"REST\"\n",
        "        percentage = (count / total_samples) * 100\n",
        "        print(f\"  {label} (target={value}): {count:,} ({percentage:.2f}%)\")\n",
        "    \n",
        "    # Temporal distribution analysis\n",
        "    print(f\"\\n📅 Temporal Distribution:\")\n",
        "    y_data_with_date = y_data.copy()\n",
        "    y_data_with_date['year'] = y_data_with_date.index.year\n",
        "    y_data_with_date['month'] = y_data_with_date.index.month\n",
        "    y_data_with_date['year_month'] = y_data_with_date.index.to_period('M')\n",
        "    \n",
        "    # Yearly distribution\n",
        "    yearly_dist = y_data_with_date.groupby('year')['target'].agg(['count', 'sum']).reset_index()\n",
        "    yearly_dist['sell_pct'] = (yearly_dist['sum'] / yearly_dist['count']) * 100\n",
        "    yearly_dist['rest_count'] = yearly_dist['count'] - yearly_dist['sum']\n",
        "    \n",
        "    print(f\"  Yearly Distribution:\")\n",
        "    for _, row in yearly_dist.iterrows():\n",
        "        print(f\"    {int(row['year'])}: {int(row['sum'])} SELL / {int(row['rest_count'])} REST ({row['sell_pct']:.2f}% SELL)\")\n",
        "    \n",
        "    # Monthly distribution (last 12 months)\n",
        "    monthly_dist = y_data_with_date.groupby('year_month')['target'].agg(['count', 'sum']).reset_index()\n",
        "    monthly_dist['sell_pct'] = (monthly_dist['sum'] / monthly_dist['count']) * 100\n",
        "    monthly_dist['rest_count'] = monthly_dist['count'] - monthly_dist['sum']\n",
        "    \n",
        "    print(f\"\\n  Monthly Distribution (Last 12 months):\")\n",
        "    last_12_months = monthly_dist.tail(12)\n",
        "    for _, row in last_12_months.iterrows():\n",
        "        print(f\"    {row['year_month']}: {int(row['sum'])} SELL / {int(row['rest_count'])} REST ({row['sell_pct']:.2f}% SELL)\")\n",
        "    \n",
        "    # Consecutive SELL analysis\n",
        "    print(f\"\\n🔍 Consecutive SELL Analysis:\")\n",
        "    y_series = y_data['target'].values\n",
        "    consecutive_sells = []\n",
        "    current_consecutive = 0\n",
        "    \n",
        "    for i, value in enumerate(y_series):\n",
        "        if value == 1:  # SELL\n",
        "            current_consecutive += 1\n",
        "        else:  # REST\n",
        "            if current_consecutive > 0:\n",
        "                consecutive_sells.append(current_consecutive)\n",
        "                current_consecutive = 0\n",
        "    \n",
        "    if current_consecutive > 0:  # Handle case where series ends with SELL\n",
        "        consecutive_sells.append(current_consecutive)\n",
        "    \n",
        "    if consecutive_sells:\n",
        "        max_consecutive = max(consecutive_sells)\n",
        "        avg_consecutive = sum(consecutive_sells) / len(consecutive_sells)\n",
        "        print(f\"  Max consecutive SELLs: {max_consecutive}\")\n",
        "        print(f\"  Average consecutive SELLs: {avg_consecutive:.2f}\")\n",
        "        print(f\"  Total consecutive SELL sequences: {len(consecutive_sells)}\")\n",
        "    else:\n",
        "        print(f\"  No consecutive SELL sequences found\")\n",
        "    \n",
        "    # SELL clustering analysis (SELLs within 5 periods)\n",
        "    print(f\"\\n🎯 SELL Clustering Analysis:\")\n",
        "    sell_indices = y_data[y_data['target'] == 1].index\n",
        "    if len(sell_indices) > 1:\n",
        "        time_diffs = []\n",
        "        for i in range(1, len(sell_indices)):\n",
        "            diff = (sell_indices[i] - sell_indices[i-1]).total_seconds() / 3600  # hours\n",
        "            time_diffs.append(diff)\n",
        "        \n",
        "        if time_diffs:\n",
        "            avg_time_between_sells = sum(time_diffs) / len(time_diffs)\n",
        "            min_time_between_sells = min(time_diffs)\n",
        "            max_time_between_sells = max(time_diffs)\n",
        "            \n",
        "            print(f\"  Average time between SELLs: {avg_time_between_sells:.1f} hours\")\n",
        "            print(f\"  Min time between SELLs: {min_time_between_sells:.1f} hours\")\n",
        "            print(f\"  Max time between SELLs: {max_time_between_sells:.1f} hours\")\n",
        "            \n",
        "            # Count SELLs within 5 periods (20 hours for H4)\n",
        "            close_sells = sum(1 for diff in time_diffs if diff <= 20)\n",
        "            print(f\"  SELLs within 20 hours: {close_sells} ({close_sells/len(time_diffs)*100:.1f}%)\")\n",
        "    \n",
        "    return {\n",
        "        'total_samples': total_samples,\n",
        "        'sell_samples': sell_samples,\n",
        "        'rest_samples': rest_samples,\n",
        "        'sell_percentage': sell_percentage,\n",
        "        'imbalance_ratio': imbalance_ratio,\n",
        "        'yearly_distribution': yearly_dist,\n",
        "        'monthly_distribution': monthly_dist,\n",
        "        'consecutive_sells': consecutive_sells\n",
        "    }\n",
        "\n",
        "# Load and analyze target distribution\n",
        "print(\"🔍 Loading target variable for analysis...\")\n",
        "y_analysis = pd.read_parquet('../features/y.parquet')\n",
        "\n",
        "# Analyze target distribution\n",
        "target_stats = analyze_target_distribution(y_analysis, \"BTC Sell Signal Target Distribution\")\n",
        "\n",
        "# Additional visualization\n",
        "print(f\"\\n📊 Target Distribution Summary:\")\n",
        "print(f\"  This is a highly imbalanced dataset with {target_stats['sell_percentage']:.2f}% SELL signals\")\n",
        "print(f\"  Class imbalance ratio: {target_stats['imbalance_ratio']:.1f}:1\")\n",
        "print(f\"  This imbalance will require careful handling in model training\")\n",
        "print(f\"  Consider using techniques like:\")\n",
        "print(f\"    - Class weights (scale_pos_weight)\")\n",
        "print(f\"    - SMOTE or other oversampling\")\n",
        "print(f\"    - Focal Loss\")\n",
        "print(f\"    - Stratified sampling\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2.2 Implementation Complete! ✅\n",
        "\n",
        "### **Summary of CORRECTED Implementation**\n",
        "\n",
        "**Approach**: Calculate everything together, split on save step.\n",
        "\n",
        "**Steps Completed**:\n",
        "1. ✅ **Load Full Data**: Complete dataset including buffer (2020-03-01 to 2025-10-19)\n",
        "2. ✅ **Technical Indicators**: Calculated all 19 indicators per timeframe on full data\n",
        "3. ✅ **Feature Sets A0→A3**: Created incremental feature sets with proper temporal alignment\n",
        "4. ✅ **Historical Lag Features**: Created lag features using full data for complete historical context\n",
        "5. ✅ **A4 Feature Set**: Combined A3 + all historical lags (437 features)\n",
        "6. ✅ **Data Validation**: Verified feature counts and missing values\n",
        "7. ✅ **Save Clean Data**: Split clean period (2020-05-12 to 2025-09-19) only at save step\n",
        "\n",
        "### **Key Fixes Applied**:\n",
        "1. **Full Data Calculations**: All indicators and lags calculated on complete dataset\n",
        "2. **Proper Temporal Alignment**: Enhanced alignment logic with timeframe-specific offsets\n",
        "3. **Complete Historical Context**: W1 data from 2020-05-04, M1 data from 2020-05-01\n",
        "4. **No Missing Data**: Full historical context for all lag features\n",
        "5. **Clean Final Output**: Buffer data used for calculations but not stored\n",
        "\n",
        "### **Temporal Alignment Logic**:\n",
        "- **H4 timestamp 2020-05-11 00:00:00** (candle closed at 2020-05-11 04:00:00):\n",
        "  - **D1 data**: `base_timestamp - 1d` → Use 2020-05-10 00:00:00 (previous day's close) ✅\n",
        "  - **W1 data**: `base_timestamp - 1w` → Use 2020-05-04 00:00:00 (previous week's close) ✅\n",
        "  - **M1 data**: `base_timestamp - 1m` → Use 2020-04-11 00:00:00 (previous month's close) ✅\n",
        "- **Uses timeframe-specific offsets** to ensure proper temporal alignment\n",
        "- **Ensures no future data leakage** and realistic trading scenarios\n",
        "\n",
        "### **Expected Outputs**\n",
        "- **A0.parquet**: 19 features (H4 only)\n",
        "- **A1.parquet**: 38 features (H4 + D1)\n",
        "- **A2.parquet**: 57 features (H4 + D1 + W1)\n",
        "- **A3.parquet**: 76 features (H4 + D1 + W1 + M1)\n",
        "- **A4.parquet**: 437 features (A3 + all historical lags)\n",
        "\n",
        "### **Next Steps**\n",
        "- **Step 3**: Train/test split based on timeline\n",
        "- **Step 4**: Ablation Study experiments (A0→A4_Pruned)\n",
        "- **Step 5**: Results analysis and RQ answers\n",
        "\n",
        "**Ready to proceed to Step 3!** 🚀\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Testing specific combinations...\n",
            "🧪 Testing Specific Combinations\n",
            "============================================================\n",
            "\n",
            "🧪 Window=40, Upper=+10.0%, Lower=-10.0% (40w +10/-10)\n",
            "------------------------------------------------------------\n",
            "🎯 Creating target variable (40-period window, first threshold logic)...\n",
            "🔄 Calculating target labels (this might take a while)...\n",
            "   Processing 0/12403 records...\n",
            "   Processing 1000/12403 records...\n",
            "   Processing 2000/12403 records...\n",
            "   Processing 3000/12403 records...\n",
            "   Processing 4000/12403 records...\n",
            "   Processing 5000/12403 records...\n",
            "   Processing 6000/12403 records...\n",
            "   Processing 7000/12403 records...\n",
            "   Processing 8000/12403 records...\n",
            "   Processing 9000/12403 records...\n",
            "   Processing 10000/12403 records...\n",
            "   Processing 11000/12403 records...\n",
            "   Processing 12000/12403 records...\n",
            "Filtering results to the specified date range...\n",
            "✅ Target variable created:\n",
            "   Total records in focused period: 11737\n",
            "   Sell labels (target=1): 1763\n",
            "   Rest/Buy labels (target=0): 9974\n",
            "   📊 Overall SELL: 15.02% (1,763/11,737)\n",
            "   📅 Monthly consistency: 64.6% (42/65 months)\n",
            "   Avg monthly SELL: 14.96% | Std: 16.34%\n",
            "\n",
            "🧪 Window=50, Upper=+10.0%, Lower=-12.0% (50w +10/-12)\n",
            "------------------------------------------------------------\n",
            "🎯 Creating target variable (50-period window, first threshold logic)...\n",
            "🔄 Calculating target labels (this might take a while)...\n",
            "   Processing 0/12403 records...\n",
            "   Processing 1000/12403 records...\n",
            "   Processing 2000/12403 records...\n",
            "   Processing 3000/12403 records...\n",
            "   Processing 4000/12403 records...\n",
            "   Processing 5000/12403 records...\n",
            "   Processing 6000/12403 records...\n",
            "   Processing 7000/12403 records...\n",
            "   Processing 8000/12403 records...\n",
            "   Processing 9000/12403 records...\n",
            "   Processing 10000/12403 records...\n",
            "   Processing 11000/12403 records...\n",
            "   Processing 12000/12403 records...\n",
            "Filtering results to the specified date range...\n",
            "✅ Target variable created:\n",
            "   Total records in focused period: 11737\n",
            "   Sell labels (target=1): 1560\n",
            "   Rest/Buy labels (target=0): 10177\n",
            "   📊 Overall SELL: 13.29% (1,560/11,737)\n",
            "   📅 Monthly consistency: 63.1% (41/65 months)\n",
            "   Avg monthly SELL: 13.20% | Std: 16.59%\n",
            "\n",
            "📊 Summary (Specific Combinations)\n",
            "================================================================================\n",
            "Config          Overall%   MonthlyAvg%  Consistency% Std%    \n",
            "--------------------------------------------------------------------------------\n",
            "40w +10/-10        15.02%      14.96%       64.6%  16.34%\n",
            "50w +10/-12        13.29%      13.20%       63.1%  16.59%\n"
          ]
        }
      ],
      "source": [
        "# 특정 조합 테스트: Window 40 (+10/-10), Window 50 (+10/-12)\n",
        "\n",
        "def test_specific_combinations(h4_full: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    사용자가 요청한 두 조합만 테스트\n",
        "    - Window: 40, Upper: +10.0%, Lower: -10.0%\n",
        "    - Window: 50, Upper: +10.0%, Lower: -12.0%\n",
        "    \"\"\"\n",
        "    print(\"🧪 Testing Specific Combinations\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    test_configs = [\n",
        "        {\"window\": 40, \"upper\": 0.10, \"lower\": -0.10, \"name\": \"40w +10/-10\"},\n",
        "        {\"window\": 50, \"upper\": 0.10, \"lower\": -0.12, \"name\": \"50w +10/-12\"},\n",
        "    ]\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    for config in test_configs:\n",
        "        print(f\"\\n🧪 Window={config['window']}, Upper=+{config['upper']*100:.1f}%, Lower={config['lower']*100:.1f}% ({config['name']})\")\n",
        "        print(\"-\" * 60)\n",
        "        \n",
        "        try:\n",
        "            y_result = create_target_variable_first_threshold(\n",
        "                h4_full,\n",
        "                window_size=config['window'],\n",
        "                upper_threshold=config['upper'],\n",
        "                lower_threshold=config['lower'],\n",
        "                train_start='2020-05-12',\n",
        "                test_end='2025-09-19'\n",
        "            )\n",
        "            \n",
        "            # Monthly aggregation\n",
        "            monthly = {}\n",
        "            for idx, row in y_result.iterrows():\n",
        "                ym = idx.to_period('M')\n",
        "                if ym not in monthly:\n",
        "                    monthly[ym] = {\"total\": 0, \"sell\": 0}\n",
        "                monthly[ym][\"total\"] += 1\n",
        "                monthly[ym][\"sell\"] += int(row[\"target\"] == 1)\n",
        "            \n",
        "            monthly_rows = []\n",
        "            for ym in sorted(monthly.keys()):\n",
        "                total = monthly[ym][\"total\"]\n",
        "                sell = monthly[ym][\"sell\"]\n",
        "                sell_ratio = sell / total * 100 if total else 0\n",
        "                monthly_rows.append({\"ym\": ym, \"total\": total, \"sell\": sell, \"sell_ratio\": sell_ratio})\n",
        "            \n",
        "            # Summary\n",
        "            total_samples = len(y_result)\n",
        "            total_sell = int(y_result[\"target\"].sum())\n",
        "            overall_ratio = total_sell / total_samples * 100 if total_samples else 0\n",
        "            valid_months = [r for r in monthly_rows if r[\"total\"] > 0]\n",
        "            months_with_sell = sum(1 for r in valid_months if r[\"sell_ratio\"] > 0)\n",
        "            consistency = months_with_sell / len(valid_months) * 100 if valid_months else 0\n",
        "            ratios = [r[\"sell_ratio\"] for r in valid_months]\n",
        "            avg_ratio = float(np.mean(ratios)) if ratios else 0.0\n",
        "            std_ratio = float(np.std(ratios)) if ratios else 0.0\n",
        "            \n",
        "            print(f\"   📊 Overall SELL: {overall_ratio:.2f}% ({total_sell:,}/{total_samples:,})\")\n",
        "            print(f\"   📅 Monthly consistency: {consistency:.1f}% ({months_with_sell}/{len(valid_months)} months)\")\n",
        "            print(f\"   Avg monthly SELL: {avg_ratio:.2f}% | Std: {std_ratio:.2f}%\")\n",
        "            \n",
        "            results.append({\n",
        "                \"window\": config['window'],\n",
        "                \"upper\": config['upper'],\n",
        "                \"lower\": config['lower'],\n",
        "                \"name\": config['name'],\n",
        "                \"overall_ratio\": overall_ratio,\n",
        "                \"consistency\": consistency,\n",
        "                \"avg_monthly\": avg_ratio,\n",
        "                \"std_monthly\": std_ratio,\n",
        "            })\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"   ❌ Error: {e}\")\n",
        "            continue\n",
        "    \n",
        "    # Summary table\n",
        "    print(\"\\n📊 Summary (Specific Combinations)\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"{'Config':<15} {'Overall%':<10} {'MonthlyAvg%':<12} {'Consistency%':<12} {'Std%':<8}\")\n",
        "    print(\"-\" * 80)\n",
        "    for r in results:\n",
        "        print(f\"{r['name']:<15} {r['overall_ratio']:>8.2f}% {r['avg_monthly']:>10.2f}% {r['consistency']:>10.1f}% {r['std_monthly']:>6.2f}%\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"🚀 Testing specific combinations...\")\n",
        "specific_results = test_specific_combinations(h4_full)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>open</th>\n",
              "      <th>high</th>\n",
              "      <th>low</th>\n",
              "      <th>close</th>\n",
              "      <th>volume</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>timestamp</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2025-10-27 08:00:00</th>\n",
              "      <td>115554.59</td>\n",
              "      <td>115607.48</td>\n",
              "      <td>114814.69</td>\n",
              "      <td>115362.02</td>\n",
              "      <td>3701.92326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-10-27 12:00:00</th>\n",
              "      <td>115362.02</td>\n",
              "      <td>115437.48</td>\n",
              "      <td>114503.99</td>\n",
              "      <td>114969.68</td>\n",
              "      <td>4493.43933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-10-27 16:00:00</th>\n",
              "      <td>114969.68</td>\n",
              "      <td>115790.00</td>\n",
              "      <td>114790.81</td>\n",
              "      <td>114942.64</td>\n",
              "      <td>2258.20754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-10-27 20:00:00</th>\n",
              "      <td>114942.64</td>\n",
              "      <td>114942.64</td>\n",
              "      <td>113830.01</td>\n",
              "      <td>114107.65</td>\n",
              "      <td>2236.06411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-10-28 00:00:00</th>\n",
              "      <td>114107.65</td>\n",
              "      <td>114547.20</td>\n",
              "      <td>113777.01</td>\n",
              "      <td>113777.01</td>\n",
              "      <td>1435.83458</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                          open       high        low      close      volume\n",
              "timestamp                                                                  \n",
              "2025-10-27 08:00:00  115554.59  115607.48  114814.69  115362.02  3701.92326\n",
              "2025-10-27 12:00:00  115362.02  115437.48  114503.99  114969.68  4493.43933\n",
              "2025-10-27 16:00:00  114969.68  115790.00  114790.81  114942.64  2258.20754\n",
              "2025-10-27 20:00:00  114942.64  114942.64  113830.01  114107.65  2236.06411\n",
              "2025-10-28 00:00:00  114107.65  114547.20  113777.01  113777.01  1435.83458"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "h4_full.tail(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "💾 Saving focused feature sets...\n",
            "🎯 Creating target variable (50-period window, first threshold logic)...\n",
            "🔄 Calculating target labels (this might take a while)...\n",
            "   Processing 0/12403 records...\n",
            "   Processing 1000/12403 records...\n",
            "   Processing 2000/12403 records...\n",
            "   Processing 3000/12403 records...\n",
            "   Processing 4000/12403 records...\n",
            "   Processing 5000/12403 records...\n",
            "   Processing 6000/12403 records...\n",
            "   Processing 7000/12403 records...\n",
            "   Processing 8000/12403 records...\n",
            "   Processing 9000/12403 records...\n",
            "   Processing 10000/12403 records...\n",
            "   Processing 11000/12403 records...\n",
            "   Processing 12000/12403 records...\n",
            "Filtering results to the specified date range...\n",
            "✅ Target variable created:\n",
            "   Total records in focused period: 11737\n",
            "   Sell labels (target=1): 1560\n",
            "   Rest/Buy labels (target=0): 10177\n",
            "🎉 All feature sets saved successfully!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>timestamp</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2020-05-12 00:00:00</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-05-12 04:00:00</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-05-12 08:00:00</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-05-12 12:00:00</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2020-05-12 16:00:00</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-09-18 08:00:00</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-09-18 12:00:00</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-09-18 16:00:00</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-09-18 20:00:00</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2025-09-19 00:00:00</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11737 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                     target\n",
              "timestamp                  \n",
              "2020-05-12 00:00:00       0\n",
              "2020-05-12 04:00:00       0\n",
              "2020-05-12 08:00:00       0\n",
              "2020-05-12 12:00:00       0\n",
              "2020-05-12 16:00:00       0\n",
              "...                     ...\n",
              "2025-09-18 08:00:00       0\n",
              "2025-09-18 12:00:00       0\n",
              "2025-09-18 16:00:00       0\n",
              "2025-09-18 20:00:00       0\n",
              "2025-09-19 00:00:00       0\n",
              "\n",
              "[11737 rows x 1 columns]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def create_and_save_y(h4_full, train_start='2020-05-12', test_end='2025-09-19'):\n",
        "    \"\"\"Save focused feature sets with correct target variable creation\"\"\"\n",
        "    \n",
        "    print(\"💾 Saving focused feature sets...\")\n",
        "    \n",
        "    # Create features directory\n",
        "    features_dir = Path('../features')\n",
        "    features_dir.mkdir(exist_ok=True)\n",
        "    \n",
        "    # Create target variable using FULL H4 data\n",
        "    y_focused = create_target_variable_first_threshold(h4_full,\n",
        "        window_size=50,\n",
        "        upper_threshold=0.10,\n",
        "        lower_threshold=-0.12,\n",
        "        train_start=train_start, test_end=test_end)\n",
        "    \n",
        "    # Save all files\n",
        "    y_focused.to_parquet(features_dir / 'y.parquet')\n",
        "    \n",
        "    print(\"🎉 All feature sets saved successfully!\")\n",
        "    return y_focused\n",
        "\n",
        "# Run the complete save function\n",
        "create_and_save_y(h4_full, train_start='2020-05-12', test_end='2025-09-19')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "csml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
