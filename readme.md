# BTC 'Sell' Signal Prediction - Ablation Study

## ğŸ¯ Project Overview

This project conducts a **systematic Ablation Study** to identify optimal Multi-Timeframe (MTF) and Historical Lag feature combinations for BTC 'Sell' signal prediction. The goal is to answer two key research questions through controlled experiments.

**Research Questions:**
- **RQ1 (Current timepoint):** What is the optimal MTF combination when expanding H4â†’D1â†’W1â†’M1?
- **RQ2 (Historical timepoint):** Do systematic historical lag features improve prediction?

## ğŸ—ï¸ Architecture

**Stacking Ensemble for Fair Comparison:**
- **Level 0 Models:** XGBoost, Random Forest, Logistic Regression
- **Level 1 Meta-Model:** Logistic Regression
- **Validation:** TimeSeriesSplit (n_splits=5) with fallback to Rolling Window
- **Target:** Binary classification (Sell vs Rest) - 30-day -15% price drops
- **Experiments:** A0 (Baseline) â†’ A1 (MTF-1) â†’ A2 (MTF-2) â†’ A3 (MTF-3) â†’ A4_Pruned (Historical)

## ğŸš€ Key Features

- **Systematic Ablation:** Controlled experiments to isolate MTF and historical contributions
- **Modular Design:** Separate Python modules for each model component
- **Advanced Tuning:** Bayesian optimization for complex models, GridSearchCV for simple models
- **Time Series Aware:** Proper handling of temporal data with no data leakage
- **Feature Pruning:** XGBoost-based importance selection for A4_Pruned
- **Research Focus:** Data-driven answers to specific research questions

## ğŸ“ Project Structure

```
btc_prediction/
â”œâ”€â”€ data_collection/              # Step 1: Raw data collection
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â””â”€â”€ btc_collector.py
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ exchange_config.py
â”‚   â”œâ”€â”€ data/                     # Raw OHLCV data (H4, D1, W1, M1)
â”‚   â””â”€â”€ logs/
â”œâ”€â”€ config/                       # Global configuration
â”‚   â”œâ”€â”€ settings.py               # Data paths, constants
â”‚   â””â”€â”€ model_params.py           # Hyperparameter grids
â”œâ”€â”€ data_processing/              # Step 2: Feature engineering
â”‚   â”œâ”€â”€ 01_data_collector.py      # Data collection wrapper
â”‚   â””â”€â”€ 02_feature_engineer.py    # Create A0-A4 feature sets
â”œâ”€â”€ training/                     # Steps 4-5: Experiment execution
â”‚   â”œâ”€â”€ 03_run_experiment.py      # Main experiment runner (takes exp_id)
â”‚   â”œâ”€â”€ train_l0.py               # L0 model training utilities
â”‚   â””â”€â”€ train_l1.py               # L1 meta-model training utilities
â”œâ”€â”€ evaluation/                   # Step 5: Results analysis
â”‚   â””â”€â”€ 04_evaluate_results.py    # Analyze experiment_results.csv
â”œâ”€â”€ models/                       # Step 3: Model implementations
â”‚   â”œâ”€â”€ level0/
â”‚   â”‚   â”œâ”€â”€ xgboost_model.py
â”‚   â”‚   â”œâ”€â”€ random_forest_model.py
â”‚   â”‚   â””â”€â”€ logistic_regression_model.py
â”‚   â”œâ”€â”€ level1/
â”‚   â”‚   â””â”€â”€ meta_model.py
â”‚   â””â”€â”€ ensemble/
â”‚       â””â”€â”€ stacking_ensemble.py
â”œâ”€â”€ utils/                        # Shared utilities
â”‚   â”œâ”€â”€ data_utils.py
â”‚   â”œâ”€â”€ cv_utils.py               # TimeSeriesSplit utilities
â”‚   â””â”€â”€ evaluation_utils.py
â”œâ”€â”€ notebooks/                    # Jupyter notebooks for exploration
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_feature_engineering.ipynb
â”‚   â””â”€â”€ 03_results_analysis.ipynb
â”œâ”€â”€ logs/                         # Training logs and artifacts
â”‚   â”œâ”€â”€ experiment_results.csv    # Main results table
â”‚   â””â”€â”€ models/                   # Saved model artifacts
â”œâ”€â”€ features/                     # Step 2 output: Feature sets
â”‚   â”œâ”€â”€ A0.parquet
â”‚   â”œâ”€â”€ A1.parquet
â”‚   â”œâ”€â”€ A2.parquet
â”‚   â”œâ”€â”€ A3.parquet
â”‚   â”œâ”€â”€ A4.parquet
â”‚   â”œâ”€â”€ A4_Pruned.parquet
â”‚   â””â”€â”€ y.parquet
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ plan.md                       # Implementation plan
â””â”€â”€ README.md                     # This file
```

## ğŸ”¬ Ablation Study Workflow

1. **Data Collection (Step 1):** Collect H4, D1, W1, M1 timeframes (2020-03-01 to 2025-10-19)
2. **Feature Engineering (Step 2):** Create 5 feature sets (A0â†’A4)
3. **Environment Setup (Step 3):** Build modular codebase
4. **Experiment Loop (Step 4):** Run A0â†’A1â†’A2â†’A3â†’A4_Pruned sequentially
5. **Analysis (Step 5):** Answer RQ1 (MTF) and RQ2 (Historical lags)

## ğŸ› ï¸ Technical Stack

- **Data Collection:** ccxt
- **ML Libraries:** scikit-learn, xgboost, scikit-optimize
- **Interpretability:** shap
- **Development:** Python 3.13, Jupyter
- **Environment:** Conda environment "csml"

## ğŸš€ Getting Started

### Environment Setup
```bash
# Create conda environment with Python 3.13
conda create -n csml python=3.13
conda activate csml

# Install requirements
pip install -r requirements.txt
```

### Running Ablation Study

```bash
# Step 1: Collect data (if not done)
python data_collection/scripts/btc_collector.py

# Step 2: Create feature sets
python data_processing/02_feature_engineer.py

# Step 3: Run ablation experiments
python training/03_run_experiment.py --exp_id A0
python training/03_run_experiment.py --exp_id A1
python training/03_run_experiment.py --exp_id A2
python training/03_run_experiment.py --exp_id A3

# Step 4: Feature pruning and final experiment
python training/03_run_experiment.py --prune_a4  # Creates A4_Pruned.parquet
python training/03_run_experiment.py --exp_id A4_Pruned

# Step 5: Analyze results
python evaluation/04_evaluate_results.py
```

## ğŸ“ˆ Results

*Results will be documented here after model training and evaluation.*

## ğŸ¯ Conclusion

*Conclusion will be added after project completion.*


